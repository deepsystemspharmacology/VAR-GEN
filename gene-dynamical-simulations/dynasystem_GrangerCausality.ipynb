{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    Y_t = \\sum_{i=1}^k \\alpha_{i} Y_{t-i}  + \\sum_{i=1}^k \\beta_{i} X_{t-i} + \\epsilon_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEnCAYAAAC3/AQgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1060db550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scratch work\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "u = np.random.normal(loc=12, scale=1.5, size=100)\n",
    "s = np.random.normal(loc=10, scale=2.0, size=100)\n",
    "bins = np.linspace(6, 16, 15)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(u, alpha=0.5, color = 'red', label=\"Unshuffled\", bins=bins )\n",
    "plt.hist(s, alpha=0.5, color = 'blue', label=\"Shuffled\", bins=bins)\n",
    "plt.xlim(4,20)\n",
    "plt.ylim(0, 24)\n",
    "plt.xlabel(\"Causal Value\", fontsize=20)\n",
    "plt.axvline(x = 14, label=\"Threshold\", color='black', linestyle='dashed', linewidth=3.0)\n",
    "plt.title(\"FDR Control\", fontsize=20)\n",
    "plt.ylabel(\"Count\", fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{ \\text{# Shuffled }}{\\text{# Shuffled + # Unshuffled}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as io\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot for Z -> X -> Y: Pairwise Granger Causality\n",
    "Edited output figure format/directory: 4/9/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPGC.R\u001b[m\u001b[m                              \u001b[31mPGC_run_VARS_3_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_VAR.r\u001b[m\u001b[m                          \u001b[31mPGC_run_VARS_4_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_1_.csv\u001b[m\u001b[m                       \u001b[31mPGC_run_VARS_5_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_1_.mat\u001b[m\u001b[m                       \u001b[31mPGC_run_VARS_6_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_2_.csv\u001b[m\u001b[m                       \u001b[31mPGC_run_VARS_7_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_2_.mat\u001b[m\u001b[m                       \u001b[31mPGC_run_VARS_8_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_3_.csv\u001b[m\u001b[m                       \u001b[31mPGC_run_VARS_9_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_3_.mat\u001b[m\u001b[m                       \u001b[31mPGCrun.R\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_4_.csv\u001b[m\u001b[m                       \u001b[31mPGCrun_VAR.r\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_4_.mat\u001b[m\u001b[m                       \u001b[31mexpression_ZXYA_200000_Z->X->Y.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_5_.csv\u001b[m\u001b[m                       \u001b[31mpython_PGC_p_1_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_5_.mat\u001b[m\u001b[m                       \u001b[31mpython_PGC_p_2_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_6_.csv\u001b[m\u001b[m                       \u001b[31mpython_PGC_p_3_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_6_.mat\u001b[m\u001b[m                       \u001b[31mpython_PGC_p_4_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_7_.csv\u001b[m\u001b[m                       \u001b[31mpython_PGC_p_5_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_7_.mat\u001b[m\u001b[m                       \u001b[31mpython_PGC_p_6_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_8_.csv\u001b[m\u001b[m                       \u001b[31mpython_PGC_p_7_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_8_.mat\u001b[m\u001b[m                       \u001b[31mpython_PGC_p_8_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_9_.csv\u001b[m\u001b[m                       \u001b[31mpython_PGC_p_9_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_9_.mat\u001b[m\u001b[m                       \u001b[31mworkspace.RData\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_run_VARS_1_.mat\u001b[m\u001b[m                \u001b[31mworkspace_B.RData\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_run_VARS_2_.mat\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "gcdir =  'granger-causality/ZactXactY_R/PGC/'\n",
    "model = 'Z->X->Y'\n",
    "model_orders = range(1,10)\n",
    "outfile = \"for_figures/plots/\" + model.replace(\"->\", \"act\") + \"_\" + \"p_matrix.pdf\"\n",
    "#outfile = outfile[:-4] + \".png\"\n",
    "savefig = True\n",
    "\n",
    "\n",
    "\n",
    "!ls $gcdir\n",
    "filelist = [gcdir + 'PGC_p_' + str(i) + \"_.mat\" for i in model_orders]\n",
    "\n",
    "matrices_dict = collections.OrderedDict()\n",
    "\n",
    "for matr_file in filelist:\n",
    "    mat_dict = io.loadmat(matr_file, appendmat=False)\n",
    "    matrices_dict[matr_file] = mat_dict[mat_dict.keys()[0]]\n",
    "    matrices_dict[matr_file][np.where(matrices_dict[matr_file] == 0)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value is  6\n",
      "Min value is  2\n",
      "Saved to  for_figures/plots/ZactXactY_p_matrix.pdf\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAFmCAYAAADXkmU6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuiVEKgiCIKhbQqSCIAiCuuUfX6zFh8zQ600AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106f20350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the p-values of interactions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "classes = ['Z', 'X', 'Y', 'A']\n",
    "cmap=copy.deepcopy(plt.cm.Blues)\n",
    "cmap.set_over(\"k\")\n",
    "cmap.set_under(\"w\")\n",
    "\n",
    "\n",
    "max_value = 6 #int(np.max(-np.log10(np.array(matrices_dict.values()))))\n",
    "print \"Max value is \", max_value\n",
    "min_value = 2\n",
    "print \"Min value is \", min_value\n",
    "# do a bonferonni correction\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "fig_ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "\n",
    "# fig.text(0.5, 0.98, 'Granger Causality on ' + str(model), ha='center', fontsize=25)\n",
    "# plt.ylabel('Cause Gene', fontsize=15)\n",
    "# plt.xlabel('Effect Gene', fontsize=15)\n",
    "# plt.title(\"Granger Causality of \" + str(model), fontsize=25)\n",
    "\n",
    "for i in range(len(model_orders)):\n",
    "    p = model_orders[i]\n",
    "    matr_file = matrices_dict.keys()[i]\n",
    "    \n",
    "    \n",
    "    title = str(p)\n",
    "    \n",
    "    \n",
    "    # First normalize into recalls.\n",
    "    cm = -np.log10(matrices_dict[matr_file])\n",
    "    \n",
    "    ax = plt.subplot(3,3, i+1)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap, clim=(min_value, max_value))\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=14)\n",
    "    plt.yticks(tick_marks, classes, fontsize=14)\n",
    "\n",
    "    \n",
    "position = fig.add_axes([0.85, 0.08, 0.02, 0.85])\n",
    "# cax, kw = mpl.colorbar.make_axes(fig_ax)\n",
    "\n",
    "cb = mpl.colorbar.colorbar_factory(position, im, extend=\"both\")\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "cb.set_label(\"- log p-value\", rotation=90, fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.1, right=0.85, top=0.9, bottom=0.12)\n",
    "fig.text(0.48, 0.01, 'Effect Gene', ha='center', fontsize=20)\n",
    "fig.text(0.03, 0.5, 'Causal Gene', va='center', rotation='vertical', fontsize=20)\n",
    "\n",
    "\n",
    "if savefig:\n",
    "    print \"Saved to \", outfile\n",
    "    fig.savefig(outfile)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot for Z -> X, Z -> Y, pairwise granger causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPGC.R\u001b[m\u001b[m                                \u001b[31mPGC_p_6_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_1_.mat\u001b[m\u001b[m                         \u001b[31mPGC_p_7_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_2_.mat\u001b[m\u001b[m                         \u001b[31mPGC_p_8_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_3_.mat\u001b[m\u001b[m                         \u001b[31mPGC_p_9_.mat\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_4_.mat\u001b[m\u001b[m                         \u001b[31mPGCrun.R\u001b[m\u001b[m\r\n",
      "\u001b[31mPGC_p_5_.mat\u001b[m\u001b[m                         \u001b[31mexpression_ZXYA_200000_Z->X-Z->Y.mat\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as io\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "\n",
    "gcdir =  'granger-causality/ZactX-ZactY_R/PGC/'\n",
    "model = 'Z->X-Z->Y'\n",
    "model_orders = range(1,10)\n",
    "outfile = \"for_figures/plots/\" + model.replace(\"->\", \"act\") + \"_\" + \"p_matrix.pdf\"\n",
    "savefig = True\n",
    "\n",
    "\n",
    "!ls $gcdir\n",
    "filelist = [gcdir + 'PGC_p_' + str(i) + \"_.mat\" for i in model_orders]\n",
    "\n",
    "matrices_dict = collections.OrderedDict()\n",
    "\n",
    "for matr_file in filelist:\n",
    "    mat_dict = io.loadmat(matr_file, appendmat=False)\n",
    "    matrices_dict[matr_file] = mat_dict[mat_dict.keys()[0]]\n",
    "    matrices_dict[matr_file][np.where(matrices_dict[matr_file] == 0)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value is  6\n",
      "Min value is  2\n",
      "Saved to  for_figures/plots/ZactX-ZactY_p_matrix.pdf\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAFmCAYAAADXkmU6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1069f7e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the p-values of interactions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "classes = ['Z', 'X', 'Y', 'A']\n",
    "cmap=copy.deepcopy(plt.cm.Blues)\n",
    "cmap.set_over(\"k\")\n",
    "cmap.set_under(\"w\")\n",
    "\n",
    "\n",
    "max_value = 6 #int(np.max(-np.log10(np.array(matrices_dict.values()))))\n",
    "print \"Max value is \", max_value\n",
    "min_value = 2\n",
    "print \"Min value is \", min_value\n",
    "# do a bonferonni correction\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "fig_ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "\n",
    "# fig.text(0.5, 0.98, 'Granger Causality on ' + str(model), ha='center', fontsize=25)\n",
    "# plt.ylabel('Cause Gene', fontsize=15)\n",
    "# plt.xlabel('Effect Gene', fontsize=15)\n",
    "# plt.title(\"Granger Causality of \" + str(model), fontsize=25)\n",
    "\n",
    "for i in range(len(model_orders)):\n",
    "    p = model_orders[i]\n",
    "    matr_file = matrices_dict.keys()[i]\n",
    "    \n",
    "    \n",
    "    title = str(p)\n",
    "    \n",
    "    \n",
    "    # First normalize into recalls.\n",
    "    cm = -np.log10(matrices_dict[matr_file])\n",
    "    \n",
    "    ax = plt.subplot(3,3, i+1)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap, clim=(min_value, max_value))\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=14)\n",
    "    plt.yticks(tick_marks, classes, fontsize=14)\n",
    "\n",
    "    \n",
    "position = fig.add_axes([0.85, 0.08, 0.02, 0.85])\n",
    "# cax, kw = mpl.colorbar.make_axes(fig_ax)\n",
    "\n",
    "cb = mpl.colorbar.colorbar_factory(position, im, extend=\"both\")\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "cb.set_label(\"- log p-value\", rotation=90, fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.1, right=0.85, top=0.9, bottom=0.12)\n",
    "fig.text(0.48, 0.01, 'Effect Gene', ha='center', fontsize=20)\n",
    "fig.text(0.03, 0.5, 'Causal Gene', va='center', rotation='vertical', fontsize=20)\n",
    "\n",
    "\n",
    "if savefig:\n",
    "    print \"Saved to \", outfile\n",
    "    fig.savefig(outfile)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRANGER CAUSALITY\n",
    "\n",
    "from sklearn import linear_model\n",
    "import scipy.stats as stats\n",
    "import itertools\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def get_lagged_vector(X_list, index, tau, E):\n",
    "    \"\"\"Generate lagged vector of X: (X[index], X[index - tau], ..., X[index - tau*E - 1]).\n",
    "    \n",
    "    :param X_list: list of X observations at equally spaced time intervals.\n",
    "    :param index: index to start from\n",
    "    :param tau: time lag\n",
    "    :param E: dimension of lagged vector\n",
    "    \n",
    "    :return: (X[index], X[index - tau], ..., X[index - tau*E - 1])\n",
    "    \"\"\"\n",
    "    assert index >= tau * (E - 1)\n",
    "    return [X_list[index - e * tau] for e in range(E)]\n",
    "\n",
    "\n",
    "def pairwise_granger_causality(i, j, p):\n",
    "    \"\"\"\n",
    "    Computes beta of fit and p-value of fit to determine if i granger causes j.\n",
    "    i and j must have length at least p (to construct lagged vectors)\n",
    "    Returns: beta of i on j, pvalue\n",
    "    \"\"\"\n",
    "    if len(i) != len(j):\n",
    "        raise ValueError(\"i and j must have same length\")\n",
    "    if len(i) < p:\n",
    "        raise ValueError(\"i and j must have at least length p\")\n",
    "    \n",
    "    T = len(i)\n",
    "    #lagged_i goes from p up to T, the length of i. It is a T-p matrix\n",
    "    lagged_i = np.array([get_lagged_vector(i, index, 1, p) for index in range(p-1, T-1)])\n",
    "    \n",
    "    lagged_j = np.array([get_lagged_vector(j, index, 1, p) for index in range(p-1, T-1)])\n",
    "    \n",
    "    target = np.array(j[p:T])\n",
    "    \n",
    "    print \"Length of vectors is \", T\n",
    "    print \"Model order is \", p\n",
    "    print \"shape of lagged vectors is \", lagged_i.shape\n",
    "    print \"Shapre of target is \", target.shape\n",
    "    \n",
    "    # Restricted classification\n",
    "    clf_j = linear_model.LinearRegression()\n",
    "    clf_j.fit(lagged_j, target)\n",
    "    pred_j = clf_j.predict(lagged_j)\n",
    "    \n",
    "    SSE_j = np.sum(np.power(target - pred_j, 2))\n",
    "    \n",
    "    # Unrestricted classification\n",
    "    clf_ij = linear_model.LinearRegression()\n",
    "    lagged_ij = np.concatenate((lagged_i, lagged_j), axis=1)\n",
    "    clf_ij.fit(lagged_ij, target)\n",
    "    pred_ij = clf_ij.predict(lagged_ij)\n",
    "    \n",
    "    SSE_ij = np.sum(np.power(target - pred_ij, 2))\n",
    "\n",
    "    \n",
    "    # Formula is taken from: http://pages.uoregon.edu/aarong/teaching/G4075_Outline/node4.html\n",
    "    # The variance explained by i itself compared to variance of just j\n",
    "    # IF i explains more variance then we reject\n",
    "    #DOF of first is p = 2p - p = # in ij fit - # in j fit (# of params added)\n",
    "    # DOF of bottom is is n - p - 1 (where p is the number of parameters in the bottom model)\n",
    "    F = ((SSE_j - SSE_ij) * 1.0 / p)/(SSE_ij/(len(target) - 2 * p - 1))\n",
    "\n",
    "    p_value = 1.0 - stats.f.cdf(F, p, len(target) - 2 * p - 1)\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "def pairwise_granger_causality_all(matr, p):\n",
    "    \"\"\"\n",
    "    Assume matr is an n by T matrix, where n is number of genes, T is # timepoints.\n",
    "    \"\"\"\n",
    "    n, T = np.array(matr).shape\n",
    "    \n",
    "    new_matr = np.zeros(shape=(n,n))\n",
    "    \n",
    "    pairs = itertools.permutations(range(n), 2)\n",
    "    \n",
    "    for pair in pairs:\n",
    "        i, j = pair\n",
    "        new_matr[i][j] = pairwise_granger_causality(matr[i], matr[j], p)\n",
    "        \n",
    "    return new_matr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 111)\n",
      "Length of vectors is  111\n",
      "Model order is  1\n",
      "shape of lagged vectors is  (110, 1)\n",
      "Shapre of target is  (110,)\n",
      "Length of vectors is  111\n",
      "Model order is  1\n",
      "shape of lagged vectors is  (110, 1)\n",
      "Shapre of target is  (110,)\n",
      "Length of vectors is  111\n",
      "Model order is  1\n",
      "shape of lagged vectors is  (110, 1)\n",
      "Shapre of target is  (110,)\n",
      "Length of vectors is  111\n",
      "Model order is  1\n",
      "shape of lagged vectors is  (110, 1)\n",
      "Shapre of target is  (110,)\n",
      "Length of vectors is  111\n",
      "Model order is  1\n",
      "shape of lagged vectors is  (110, 1)\n",
      "Shapre of target is  (110,)\n",
      "Length of vectors is  111\n",
      "Model order is  1\n",
      "shape of lagged vectors is  (110, 1)\n",
      "Shapre of target is  (110,)\n",
      "Length of vectors is  111\n",
      "Model order is  1\n",
      "shape of lagged vectors is  (110, 1)\n",
      "Shapre of target is  (110,)\n",
      "Length of vectors is  111\n",
      "Model order is  1\n",
      "shape of lagged vectors is  (110, 1)\n",
      "Shapre of target is  (110,)\n",
      "Length of vectors is  111\n",
      "Model order is  1\n",
      "shape of lagged vectors is  (110, 1)\n",
      "Shapre of target is  (110,)\n",
      "Length of vectors is  111\n",
      "Model order is  1\n",
      "shape of lagged vectors is  (110, 1)\n",
      "Shapre of target is  (110,)\n",
      "Length of vectors is  111\n",
      "Model order is  1\n",
      "shape of lagged vectors is  (110, 1)\n",
      "Shapre of target is  (110,)\n",
      "Length of vectors is  111\n",
      "Model order is  1\n",
      "shape of lagged vectors is  (110, 1)\n",
      "Shapre of target is  (110,)\n",
      "Length of vectors is  111\n",
      "Model order is  2\n",
      "shape of lagged vectors is  (109, 2)\n",
      "Shapre of target is  (109,)\n",
      "Length of vectors is  111\n",
      "Model order is  2\n",
      "shape of lagged vectors is  (109, 2)\n",
      "Shapre of target is  (109,)\n",
      "Length of vectors is  111\n",
      "Model order is  2\n",
      "shape of lagged vectors is  (109, 2)\n",
      "Shapre of target is  (109,)\n",
      "Length of vectors is  111\n",
      "Model order is  2\n",
      "shape of lagged vectors is  (109, 2)\n",
      "Shapre of target is  (109,)\n",
      "Length of vectors is  111\n",
      "Model order is  2\n",
      "shape of lagged vectors is  (109, 2)\n",
      "Shapre of target is  (109,)\n",
      "Length of vectors is  111\n",
      "Model order is  2\n",
      "shape of lagged vectors is  (109, 2)\n",
      "Shapre of target is  (109,)\n",
      "Length of vectors is  111\n",
      "Model order is  2\n",
      "shape of lagged vectors is  (109, 2)\n",
      "Shapre of target is  (109,)\n",
      "Length of vectors is  111\n",
      "Model order is  2\n",
      "shape of lagged vectors is  (109, 2)\n",
      "Shapre of target is  (109,)\n",
      "Length of vectors is  111\n",
      "Model order is  2\n",
      "shape of lagged vectors is  (109, 2)\n",
      "Shapre of target is  (109,)\n",
      "Length of vectors is  111\n",
      "Model order is  2\n",
      "shape of lagged vectors is  (109, 2)\n",
      "Shapre of target is  (109,)\n",
      "Length of vectors is  111\n",
      "Model order is  2\n",
      "shape of lagged vectors is  (109, 2)\n",
      "Shapre of target is  (109,)\n",
      "Length of vectors is  111\n",
      "Model order is  2\n",
      "shape of lagged vectors is  (109, 2)\n",
      "Shapre of target is  (109,)\n",
      "Length of vectors is  111\n",
      "Model order is  3\n",
      "shape of lagged vectors is  (108, 3)\n",
      "Shapre of target is  (108,)\n",
      "Length of vectors is  111\n",
      "Model order is  3\n",
      "shape of lagged vectors is  (108, 3)\n",
      "Shapre of target is  (108,)\n",
      "Length of vectors is  111\n",
      "Model order is  3\n",
      "shape of lagged vectors is  (108, 3)\n",
      "Shapre of target is  (108,)\n",
      "Length of vectors is  111\n",
      "Model order is  3\n",
      "shape of lagged vectors is  (108, 3)\n",
      "Shapre of target is  (108,)\n",
      "Length of vectors is  111\n",
      "Model order is  3\n",
      "shape of lagged vectors is  (108, 3)\n",
      "Shapre of target is  (108,)\n",
      "Length of vectors is  111\n",
      "Model order is  3\n",
      "shape of lagged vectors is  (108, 3)\n",
      "Shapre of target is  (108,)\n",
      "Length of vectors is  111\n",
      "Model order is  3\n",
      "shape of lagged vectors is  (108, 3)\n",
      "Shapre of target is  (108,)\n",
      "Length of vectors is  111\n",
      "Model order is  3\n",
      "shape of lagged vectors is  (108, 3)\n",
      "Shapre of target is  (108,)\n",
      "Length of vectors is  111\n",
      "Model order is  3\n",
      "shape of lagged vectors is  (108, 3)\n",
      "Shapre of target is  (108,)\n",
      "Length of vectors is  111\n",
      "Model order is  3\n",
      "shape of lagged vectors is  (108, 3)\n",
      "Shapre of target is  (108,)\n",
      "Length of vectors is  111\n",
      "Model order is  3\n",
      "shape of lagged vectors is  (108, 3)\n",
      "Shapre of target is  (108,)\n",
      "Length of vectors is  111\n",
      "Model order is  3\n",
      "shape of lagged vectors is  (108, 3)\n",
      "Shapre of target is  (108,)\n",
      "Length of vectors is  111\n",
      "Model order is  4\n",
      "shape of lagged vectors is  (107, 4)\n",
      "Shapre of target is  (107,)\n",
      "Length of vectors is  111\n",
      "Model order is  4\n",
      "shape of lagged vectors is  (107, 4)\n",
      "Shapre of target is  (107,)\n",
      "Length of vectors is  111\n",
      "Model order is  4\n",
      "shape of lagged vectors is  (107, 4)\n",
      "Shapre of target is  (107,)\n",
      "Length of vectors is  111\n",
      "Model order is  4\n",
      "shape of lagged vectors is  (107, 4)\n",
      "Shapre of target is  (107,)\n",
      "Length of vectors is  111\n",
      "Model order is  4\n",
      "shape of lagged vectors is  (107, 4)\n",
      "Shapre of target is  (107,)\n",
      "Length of vectors is  111\n",
      "Model order is  4\n",
      "shape of lagged vectors is  (107, 4)\n",
      "Shapre of target is  (107,)\n",
      "Length of vectors is  111\n",
      "Model order is  4\n",
      "shape of lagged vectors is  (107, 4)\n",
      "Shapre of target is  (107,)\n",
      "Length of vectors is  111\n",
      "Model order is  4\n",
      "shape of lagged vectors is  (107, 4)\n",
      "Shapre of target is  (107,)\n",
      "Length of vectors is  111\n",
      "Model order is  4\n",
      "shape of lagged vectors is  (107, 4)\n",
      "Shapre of target is  (107,)\n",
      "Length of vectors is  111\n",
      "Model order is  4\n",
      "shape of lagged vectors is  (107, 4)\n",
      "Shapre of target is  (107,)\n",
      "Length of vectors is  111\n",
      "Model order is  4\n",
      "shape of lagged vectors is  (107, 4)\n",
      "Shapre of target is  (107,)\n",
      "Length of vectors is  111\n",
      "Model order is  4\n",
      "shape of lagged vectors is  (107, 4)\n",
      "Shapre of target is  (107,)\n",
      "Length of vectors is  111\n",
      "Model order is  5\n",
      "shape of lagged vectors is  (106, 5)\n",
      "Shapre of target is  (106,)\n",
      "Length of vectors is  111\n",
      "Model order is  5\n",
      "shape of lagged vectors is  (106, 5)\n",
      "Shapre of target is  (106,)\n",
      "Length of vectors is  111\n",
      "Model order is  5\n",
      "shape of lagged vectors is  (106, 5)\n",
      "Shapre of target is  (106,)\n",
      "Length of vectors is  111\n",
      "Model order is  5\n",
      "shape of lagged vectors is  (106, 5)\n",
      "Shapre of target is  (106,)\n",
      "Length of vectors is  111\n",
      "Model order is  5\n",
      "shape of lagged vectors is  (106, 5)\n",
      "Shapre of target is  (106,)\n",
      "Length of vectors is  111\n",
      "Model order is  5\n",
      "shape of lagged vectors is  (106, 5)\n",
      "Shapre of target is  (106,)\n",
      "Length of vectors is  111\n",
      "Model order is  5\n",
      "shape of lagged vectors is  (106, 5)\n",
      "Shapre of target is  (106,)\n",
      "Length of vectors is  111\n",
      "Model order is  5\n",
      "shape of lagged vectors is  (106, 5)\n",
      "Shapre of target is  (106,)\n",
      "Length of vectors is  111\n",
      "Model order is  5\n",
      "shape of lagged vectors is  (106, 5)\n",
      "Shapre of target is  (106,)\n",
      "Length of vectors is  111\n",
      "Model order is  5\n",
      "shape of lagged vectors is  (106, 5)\n",
      "Shapre of target is  (106,)\n",
      "Length of vectors is  111\n",
      "Model order is  5\n",
      "shape of lagged vectors is  (106, 5)\n",
      "Shapre of target is  (106,)\n",
      "Length of vectors is  111\n",
      "Model order is  5\n",
      "shape of lagged vectors is  (106, 5)\n",
      "Shapre of target is  (106,)\n",
      "Length of vectors is  111\n",
      "Model order is  6\n",
      "shape of lagged vectors is  (105, 6)\n",
      "Shapre of target is  (105,)\n",
      "Length of vectors is  111\n",
      "Model order is  6\n",
      "shape of lagged vectors is  (105, 6)\n",
      "Shapre of target is  (105,)\n",
      "Length of vectors is  111\n",
      "Model order is  6\n",
      "shape of lagged vectors is  (105, 6)\n",
      "Shapre of target is  (105,)\n",
      "Length of vectors is  111\n",
      "Model order is  6\n",
      "shape of lagged vectors is  (105, 6)\n",
      "Shapre of target is  (105,)\n",
      "Length of vectors is  111\n",
      "Model order is  6\n",
      "shape of lagged vectors is  (105, 6)\n",
      "Shapre of target is  (105,)\n",
      "Length of vectors is  111\n",
      "Model order is  6\n",
      "shape of lagged vectors is  (105, 6)\n",
      "Shapre of target is  (105,)\n",
      "Length of vectors is  111\n",
      "Model order is  6\n",
      "shape of lagged vectors is  (105, 6)\n",
      "Shapre of target is  (105,)\n",
      "Length of vectors is  111\n",
      "Model order is  6\n",
      "shape of lagged vectors is  (105, 6)\n",
      "Shapre of target is  (105,)\n",
      "Length of vectors is  111\n",
      "Model order is  6\n",
      "shape of lagged vectors is  (105, 6)\n",
      "Shapre of target is  (105,)\n",
      "Length of vectors is  111\n",
      "Model order is  6\n",
      "shape of lagged vectors is  (105, 6)\n",
      "Shapre of target is  (105,)\n",
      "Length of vectors is  111\n",
      "Model order is  6\n",
      "shape of lagged vectors is  (105, 6)\n",
      "Shapre of target is  (105,)\n",
      "Length of vectors is  111\n",
      "Model order is  6\n",
      "shape of lagged vectors is  (105, 6)\n",
      "Shapre of target is  (105,)\n",
      "Length of vectors is  111\n",
      "Model order is  7\n",
      "shape of lagged vectors is  (104, 7)\n",
      "Shapre of target is  (104,)\n",
      "Length of vectors is  111\n",
      "Model order is  7\n",
      "shape of lagged vectors is  (104, 7)\n",
      "Shapre of target is  (104,)\n",
      "Length of vectors is  111\n",
      "Model order is  7\n",
      "shape of lagged vectors is  (104, 7)\n",
      "Shapre of target is  (104,)\n",
      "Length of vectors is  111\n",
      "Model order is  7\n",
      "shape of lagged vectors is  (104, 7)\n",
      "Shapre of target is  (104,)\n",
      "Length of vectors is  111\n",
      "Model order is  7\n",
      "shape of lagged vectors is  (104, 7)\n",
      "Shapre of target is  (104,)\n",
      "Length of vectors is  111\n",
      "Model order is  7\n",
      "shape of lagged vectors is  (104, 7)\n",
      "Shapre of target is  (104,)\n",
      "Length of vectors is  111\n",
      "Model order is  7\n",
      "shape of lagged vectors is  (104, 7)\n",
      "Shapre of target is  (104,)\n",
      "Length of vectors is  111\n",
      "Model order is  7\n",
      "shape of lagged vectors is  (104, 7)\n",
      "Shapre of target is  (104,)\n",
      "Length of vectors is  111\n",
      "Model order is  7\n",
      "shape of lagged vectors is  (104, 7)\n",
      "Shapre of target is  (104,)\n",
      "Length of vectors is  111\n",
      "Model order is  7\n",
      "shape of lagged vectors is  (104, 7)\n",
      "Shapre of target is  (104,)\n",
      "Length of vectors is  111\n",
      "Model order is  7\n",
      "shape of lagged vectors is  (104, 7)\n",
      "Shapre of target is  (104,)\n",
      "Length of vectors is  111\n",
      "Model order is  7\n",
      "shape of lagged vectors is  (104, 7)\n",
      "Shapre of target is  (104,)\n",
      "Length of vectors is  111\n",
      "Model order is  8\n",
      "shape of lagged vectors is  (103, 8)\n",
      "Shapre of target is  (103,)\n",
      "Length of vectors is  111\n",
      "Model order is  8\n",
      "shape of lagged vectors is  (103, 8)\n",
      "Shapre of target is  (103,)\n",
      "Length of vectors is  111\n",
      "Model order is  8\n",
      "shape of lagged vectors is  (103, 8)\n",
      "Shapre of target is  (103,)\n",
      "Length of vectors is  111\n",
      "Model order is  8\n",
      "shape of lagged vectors is  (103, 8)\n",
      "Shapre of target is  (103,)\n",
      "Length of vectors is  111\n",
      "Model order is  8\n",
      "shape of lagged vectors is  (103, 8)\n",
      "Shapre of target is  (103,)\n",
      "Length of vectors is  111\n",
      "Model order is  8\n",
      "shape of lagged vectors is  (103, 8)\n",
      "Shapre of target is  (103,)\n",
      "Length of vectors is  111\n",
      "Model order is  8\n",
      "shape of lagged vectors is  (103, 8)\n",
      "Shapre of target is  (103,)\n",
      "Length of vectors is  111\n",
      "Model order is  8\n",
      "shape of lagged vectors is  (103, 8)\n",
      "Shapre of target is  (103,)\n",
      "Length of vectors is  111\n",
      "Model order is  8\n",
      "shape of lagged vectors is  (103, 8)\n",
      "Shapre of target is  (103,)\n",
      "Length of vectors is  111\n",
      "Model order is  8\n",
      "shape of lagged vectors is  (103, 8)\n",
      "Shapre of target is  (103,)\n",
      "Length of vectors is  111\n",
      "Model order is  8\n",
      "shape of lagged vectors is  (103, 8)\n",
      "Shapre of target is  (103,)\n",
      "Length of vectors is  111\n",
      "Model order is  8\n",
      "shape of lagged vectors is  (103, 8)\n",
      "Shapre of target is  (103,)\n",
      "Length of vectors is  111\n",
      "Model order is  9\n",
      "shape of lagged vectors is  (102, 9)\n",
      "Shapre of target is  (102,)\n",
      "Length of vectors is  111\n",
      "Model order is  9\n",
      "shape of lagged vectors is  (102, 9)\n",
      "Shapre of target is  (102,)\n",
      "Length of vectors is  111\n",
      "Model order is  9\n",
      "shape of lagged vectors is  (102, 9)\n",
      "Shapre of target is  (102,)\n",
      "Length of vectors is  111\n",
      "Model order is  9\n",
      "shape of lagged vectors is  (102, 9)\n",
      "Shapre of target is  (102,)\n",
      "Length of vectors is  111\n",
      "Model order is  9\n",
      "shape of lagged vectors is  (102, 9)\n",
      "Shapre of target is  (102,)\n",
      "Length of vectors is  111\n",
      "Model order is  9\n",
      "shape of lagged vectors is  (102, 9)\n",
      "Shapre of target is  (102,)\n",
      "Length of vectors is  111\n",
      "Model order is  9\n",
      "shape of lagged vectors is  (102, 9)\n",
      "Shapre of target is  (102,)\n",
      "Length of vectors is  111\n",
      "Model order is  9\n",
      "shape of lagged vectors is  (102, 9)\n",
      "Shapre of target is  (102,)\n",
      "Length of vectors is  111\n",
      "Model order is  9\n",
      "shape of lagged vectors is  (102, 9)\n",
      "Shapre of target is  (102,)\n",
      "Length of vectors is  111\n",
      "Model order is  9\n",
      "shape of lagged vectors is  (102, 9)\n",
      "Shapre of target is  (102,)\n",
      "Length of vectors is  111\n",
      "Model order is  9\n",
      "shape of lagged vectors is  (102, 9)\n",
      "Shapre of target is  (102,)\n",
      "Length of vectors is  111\n",
      "Model order is  9\n",
      "shape of lagged vectors is  (102, 9)\n",
      "Shapre of target is  (102,)\n"
     ]
    }
   ],
   "source": [
    "from scipy import io\n",
    "\n",
    "\n",
    "outfilepref = \"granger-causality/ZactXactY_R/PGC/python_PGC\"\n",
    "data = io.loadmat(file_name=\"granger-causality/ZactXactY_R/PGC/expression_ZXYA_200000_Z->X->Y.mat\")[\"expression\"]\n",
    "print data.shape\n",
    "\n",
    "\n",
    "for p in range(1, 10):\n",
    "    B = pairwise_granger_causality_all(data, p)\n",
    "    \n",
    "    filename = outfilepref + \"_p_\" + str(p) + \"_.mat\"\n",
    "    \n",
    "    io.savemat(filename, {\"B_python\": B})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test new granger causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg + STD loaded\n",
      "Randomized written to  ../data/GeneExpressionData/small_TPM-randomized.txt\n",
      "        t00       t05       t1_       t2_       t3_       t4_       t5_  \\\n",
      "0  0.000466 -0.000710  0.001036  0.002446 -0.000593  0.000287 -0.001110   \n",
      "1  0.001944  0.001253 -0.000280  0.000704 -0.002513 -0.001732 -0.006850   \n",
      "2  0.004336  0.010700  0.008688 -0.002444  0.001091 -0.000022  0.001504   \n",
      "3  0.154250  0.103241  0.132005  0.167655  0.210110  0.148455  0.121075   \n",
      "4  0.107607  0.148978  0.081421  0.006955  0.029222  0.039379  0.083964   \n",
      "\n",
      "        t6_       t7_       t8_       t10       t12  \n",
      "0  0.047217 -0.000168 -0.000043  0.000595 -0.000913  \n",
      "1 -0.007547  0.001417  0.036537  0.003223  0.002965  \n",
      "2  0.000854  0.000513  0.011895 -0.001113  0.011748  \n",
      "3  0.168695  0.134241  0.111858  0.054600  0.195073  \n",
      "4  0.032541  0.095341  0.066656  0.013493  0.023869  \n",
      "        t00       t05       t1_       t2_       t3_       t4_       t5_  \\\n",
      "0 -0.000710 -0.000168  0.000287  0.000466  0.001036  0.000595 -0.001110   \n",
      "1  0.001253 -0.007547  0.003223 -0.002513  0.036537 -0.006850  0.001944   \n",
      "2  0.001504  0.011895  0.000854 -0.000022  0.000513  0.010700  0.001091   \n",
      "3  0.154250  0.167655  0.111858  0.195073  0.132005  0.121075  0.210110   \n",
      "4  0.095341  0.006955  0.083964  0.029222  0.066656  0.039379  0.023869   \n",
      "\n",
      "        t6_       t7_       t8_       t10       t12  \n",
      "0 -0.000913  0.002446 -0.000043 -0.000593  0.047217  \n",
      "1  0.002965 -0.001732  0.001417  0.000704 -0.000280  \n",
      "2 -0.002444  0.008688  0.004336 -0.001113  0.011748  \n",
      "3  0.103241  0.054600  0.134241  0.168695  0.148455  \n",
      "4  0.107607  0.013493  0.148978  0.081421  0.032541  \n"
     ]
    }
   ],
   "source": [
    "import geneTSmunging as gtm\n",
    "\n",
    "data_file = \"../data/GeneExpressionData/small_TPM.txt\"\n",
    "out_file = \"../data/GeneExpressionData/small_TPM-randomized.txt\"\n",
    "\n",
    "data = gtm.load_file_and_avg(data_file)\n",
    "\n",
    "rand_data = gtm.make_and_save_randomized_data(data, out_file)\n",
    "\n",
    "print data[gtm.timekeys].head()\n",
    "print rand_data[gtm.timekeys].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import prep_jobs as pj\n",
    "partition_pair = ((0,0), (98, 98))\n",
    "\n",
    "pairlist = pj.partition_pair_to_pairlist(partition_pair, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (0, 5),\n",
       " (0, 6),\n",
       " (0, 7),\n",
       " (0, 8),\n",
       " (0, 9),\n",
       " (0, 10),\n",
       " (0, 11),\n",
       " (0, 12),\n",
       " (0, 13),\n",
       " (0, 14),\n",
       " (0, 15),\n",
       " (0, 16),\n",
       " (0, 17),\n",
       " (0, 18),\n",
       " (0, 19),\n",
       " (0, 20),\n",
       " (0, 21),\n",
       " (0, 22),\n",
       " (0, 23),\n",
       " (0, 24),\n",
       " (0, 25),\n",
       " (0, 26),\n",
       " (0, 27),\n",
       " (0, 28),\n",
       " (0, 29),\n",
       " (0, 30),\n",
       " (0, 31),\n",
       " (0, 32),\n",
       " (0, 33),\n",
       " (0, 34),\n",
       " (0, 35),\n",
       " (0, 36),\n",
       " (0, 37),\n",
       " (0, 38),\n",
       " (0, 39),\n",
       " (0, 40),\n",
       " (0, 41),\n",
       " (0, 42),\n",
       " (0, 43),\n",
       " (0, 44),\n",
       " (0, 45),\n",
       " (0, 46),\n",
       " (0, 47),\n",
       " (0, 48),\n",
       " (0, 49),\n",
       " (0, 50),\n",
       " (0, 51),\n",
       " (0, 52),\n",
       " (0, 53),\n",
       " (0, 54),\n",
       " (0, 55),\n",
       " (0, 56),\n",
       " (0, 57),\n",
       " (0, 58),\n",
       " (0, 59),\n",
       " (0, 60),\n",
       " (0, 61),\n",
       " (0, 62),\n",
       " (0, 63),\n",
       " (0, 64),\n",
       " (0, 65),\n",
       " (0, 66),\n",
       " (0, 67),\n",
       " (0, 68),\n",
       " (0, 69),\n",
       " (0, 70),\n",
       " (0, 71),\n",
       " (0, 72),\n",
       " (0, 73),\n",
       " (0, 74),\n",
       " (0, 75),\n",
       " (0, 76),\n",
       " (0, 77),\n",
       " (0, 78),\n",
       " (0, 79),\n",
       " (0, 80),\n",
       " (0, 81),\n",
       " (0, 82),\n",
       " (0, 83),\n",
       " (0, 84),\n",
       " (0, 85),\n",
       " (0, 86),\n",
       " (0, 87),\n",
       " (0, 88),\n",
       " (0, 89),\n",
       " (0, 90),\n",
       " (0, 91),\n",
       " (0, 92),\n",
       " (0, 93),\n",
       " (0, 94),\n",
       " (0, 95),\n",
       " (0, 96),\n",
       " (0, 97),\n",
       " (0, 98),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (1, 5),\n",
       " (1, 6),\n",
       " (1, 7),\n",
       " (1, 8),\n",
       " (1, 9),\n",
       " (1, 10),\n",
       " (1, 11),\n",
       " (1, 12),\n",
       " (1, 13),\n",
       " (1, 14),\n",
       " (1, 15),\n",
       " (1, 16),\n",
       " (1, 17),\n",
       " (1, 18),\n",
       " (1, 19),\n",
       " (1, 20),\n",
       " (1, 21),\n",
       " (1, 22),\n",
       " (1, 23),\n",
       " (1, 24),\n",
       " (1, 25),\n",
       " (1, 26),\n",
       " (1, 27),\n",
       " (1, 28),\n",
       " (1, 29),\n",
       " (1, 30),\n",
       " (1, 31),\n",
       " (1, 32),\n",
       " (1, 33),\n",
       " (1, 34),\n",
       " (1, 35),\n",
       " (1, 36),\n",
       " (1, 37),\n",
       " (1, 38),\n",
       " (1, 39),\n",
       " (1, 40),\n",
       " (1, 41),\n",
       " (1, 42),\n",
       " (1, 43),\n",
       " (1, 44),\n",
       " (1, 45),\n",
       " (1, 46),\n",
       " (1, 47),\n",
       " (1, 48),\n",
       " (1, 49),\n",
       " (1, 50),\n",
       " (1, 51),\n",
       " (1, 52),\n",
       " (1, 53),\n",
       " (1, 54),\n",
       " (1, 55),\n",
       " (1, 56),\n",
       " (1, 57),\n",
       " (1, 58),\n",
       " (1, 59),\n",
       " (1, 60),\n",
       " (1, 61),\n",
       " (1, 62),\n",
       " (1, 63),\n",
       " (1, 64),\n",
       " (1, 65),\n",
       " (1, 66),\n",
       " (1, 67),\n",
       " (1, 68),\n",
       " (1, 69),\n",
       " (1, 70),\n",
       " (1, 71),\n",
       " (1, 72),\n",
       " (1, 73),\n",
       " (1, 74),\n",
       " (1, 75),\n",
       " (1, 76),\n",
       " (1, 77),\n",
       " (1, 78),\n",
       " (1, 79),\n",
       " (1, 80),\n",
       " (1, 81),\n",
       " (1, 82),\n",
       " (1, 83),\n",
       " (1, 84),\n",
       " (1, 85),\n",
       " (1, 86),\n",
       " (1, 87),\n",
       " (1, 88),\n",
       " (1, 89),\n",
       " (1, 90),\n",
       " (1, 91),\n",
       " (1, 92),\n",
       " (1, 93),\n",
       " (1, 94),\n",
       " (1, 95),\n",
       " (1, 96),\n",
       " (1, 97),\n",
       " (1, 98),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (2, 5),\n",
       " (2, 6),\n",
       " (2, 7),\n",
       " (2, 8),\n",
       " (2, 9),\n",
       " (2, 10),\n",
       " (2, 11),\n",
       " (2, 12),\n",
       " (2, 13),\n",
       " (2, 14),\n",
       " (2, 15),\n",
       " (2, 16),\n",
       " (2, 17),\n",
       " (2, 18),\n",
       " (2, 19),\n",
       " (2, 20),\n",
       " (2, 21),\n",
       " (2, 22),\n",
       " (2, 23),\n",
       " (2, 24),\n",
       " (2, 25),\n",
       " (2, 26),\n",
       " (2, 27),\n",
       " (2, 28),\n",
       " (2, 29),\n",
       " (2, 30),\n",
       " (2, 31),\n",
       " (2, 32),\n",
       " (2, 33),\n",
       " (2, 34),\n",
       " (2, 35),\n",
       " (2, 36),\n",
       " (2, 37),\n",
       " (2, 38),\n",
       " (2, 39),\n",
       " (2, 40),\n",
       " (2, 41),\n",
       " (2, 42),\n",
       " (2, 43),\n",
       " (2, 44),\n",
       " (2, 45),\n",
       " (2, 46),\n",
       " (2, 47),\n",
       " (2, 48),\n",
       " (2, 49),\n",
       " (2, 50),\n",
       " (2, 51),\n",
       " (2, 52),\n",
       " (2, 53),\n",
       " (2, 54),\n",
       " (2, 55),\n",
       " (2, 56),\n",
       " (2, 57),\n",
       " (2, 58),\n",
       " (2, 59),\n",
       " (2, 60),\n",
       " (2, 61),\n",
       " (2, 62),\n",
       " (2, 63),\n",
       " (2, 64),\n",
       " (2, 65),\n",
       " (2, 66),\n",
       " (2, 67),\n",
       " (2, 68),\n",
       " (2, 69),\n",
       " (2, 70),\n",
       " (2, 71),\n",
       " (2, 72),\n",
       " (2, 73),\n",
       " (2, 74),\n",
       " (2, 75),\n",
       " (2, 76),\n",
       " (2, 77),\n",
       " (2, 78),\n",
       " (2, 79),\n",
       " (2, 80),\n",
       " (2, 81),\n",
       " (2, 82),\n",
       " (2, 83),\n",
       " (2, 84),\n",
       " (2, 85),\n",
       " (2, 86),\n",
       " (2, 87),\n",
       " (2, 88),\n",
       " (2, 89),\n",
       " (2, 90),\n",
       " (2, 91),\n",
       " (2, 92),\n",
       " (2, 93),\n",
       " (2, 94),\n",
       " (2, 95),\n",
       " (2, 96),\n",
       " (2, 97),\n",
       " (2, 98),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (3, 4),\n",
       " (3, 5),\n",
       " (3, 6),\n",
       " (3, 7),\n",
       " (3, 8),\n",
       " (3, 9),\n",
       " (3, 10),\n",
       " (3, 11),\n",
       " (3, 12),\n",
       " (3, 13),\n",
       " (3, 14),\n",
       " (3, 15),\n",
       " (3, 16),\n",
       " (3, 17),\n",
       " (3, 18),\n",
       " (3, 19),\n",
       " (3, 20),\n",
       " (3, 21),\n",
       " (3, 22),\n",
       " (3, 23),\n",
       " (3, 24),\n",
       " (3, 25),\n",
       " (3, 26),\n",
       " (3, 27),\n",
       " (3, 28),\n",
       " (3, 29),\n",
       " (3, 30),\n",
       " (3, 31),\n",
       " (3, 32),\n",
       " (3, 33),\n",
       " (3, 34),\n",
       " (3, 35),\n",
       " (3, 36),\n",
       " (3, 37),\n",
       " (3, 38),\n",
       " (3, 39),\n",
       " (3, 40),\n",
       " (3, 41),\n",
       " (3, 42),\n",
       " (3, 43),\n",
       " (3, 44),\n",
       " (3, 45),\n",
       " (3, 46),\n",
       " (3, 47),\n",
       " (3, 48),\n",
       " (3, 49),\n",
       " (3, 50),\n",
       " (3, 51),\n",
       " (3, 52),\n",
       " (3, 53),\n",
       " (3, 54),\n",
       " (3, 55),\n",
       " (3, 56),\n",
       " (3, 57),\n",
       " (3, 58),\n",
       " (3, 59),\n",
       " (3, 60),\n",
       " (3, 61),\n",
       " (3, 62),\n",
       " (3, 63),\n",
       " (3, 64),\n",
       " (3, 65),\n",
       " (3, 66),\n",
       " (3, 67),\n",
       " (3, 68),\n",
       " (3, 69),\n",
       " (3, 70),\n",
       " (3, 71),\n",
       " (3, 72),\n",
       " (3, 73),\n",
       " (3, 74),\n",
       " (3, 75),\n",
       " (3, 76),\n",
       " (3, 77),\n",
       " (3, 78),\n",
       " (3, 79),\n",
       " (3, 80),\n",
       " (3, 81),\n",
       " (3, 82),\n",
       " (3, 83),\n",
       " (3, 84),\n",
       " (3, 85),\n",
       " (3, 86),\n",
       " (3, 87),\n",
       " (3, 88),\n",
       " (3, 89),\n",
       " (3, 90),\n",
       " (3, 91),\n",
       " (3, 92),\n",
       " (3, 93),\n",
       " (3, 94),\n",
       " (3, 95),\n",
       " (3, 96),\n",
       " (3, 97),\n",
       " (3, 98),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (4, 5),\n",
       " (4, 6),\n",
       " (4, 7),\n",
       " (4, 8),\n",
       " (4, 9),\n",
       " (4, 10),\n",
       " (4, 11),\n",
       " (4, 12),\n",
       " (4, 13),\n",
       " (4, 14),\n",
       " (4, 15),\n",
       " (4, 16),\n",
       " (4, 17),\n",
       " (4, 18),\n",
       " (4, 19),\n",
       " (4, 20),\n",
       " (4, 21),\n",
       " (4, 22),\n",
       " (4, 23),\n",
       " (4, 24),\n",
       " (4, 25),\n",
       " (4, 26),\n",
       " (4, 27),\n",
       " (4, 28),\n",
       " (4, 29),\n",
       " (4, 30),\n",
       " (4, 31),\n",
       " (4, 32),\n",
       " (4, 33),\n",
       " (4, 34),\n",
       " (4, 35),\n",
       " (4, 36),\n",
       " (4, 37),\n",
       " (4, 38),\n",
       " (4, 39),\n",
       " (4, 40),\n",
       " (4, 41),\n",
       " (4, 42),\n",
       " (4, 43),\n",
       " (4, 44),\n",
       " (4, 45),\n",
       " (4, 46),\n",
       " (4, 47),\n",
       " (4, 48),\n",
       " (4, 49),\n",
       " (4, 50),\n",
       " (4, 51),\n",
       " (4, 52),\n",
       " (4, 53),\n",
       " (4, 54),\n",
       " (4, 55),\n",
       " (4, 56),\n",
       " (4, 57),\n",
       " (4, 58),\n",
       " (4, 59),\n",
       " (4, 60),\n",
       " (4, 61),\n",
       " (4, 62),\n",
       " (4, 63),\n",
       " (4, 64),\n",
       " (4, 65),\n",
       " (4, 66),\n",
       " (4, 67),\n",
       " (4, 68),\n",
       " (4, 69),\n",
       " (4, 70),\n",
       " (4, 71),\n",
       " (4, 72),\n",
       " (4, 73),\n",
       " (4, 74),\n",
       " (4, 75),\n",
       " (4, 76),\n",
       " (4, 77),\n",
       " (4, 78),\n",
       " (4, 79),\n",
       " (4, 80),\n",
       " (4, 81),\n",
       " (4, 82),\n",
       " (4, 83),\n",
       " (4, 84),\n",
       " (4, 85),\n",
       " (4, 86),\n",
       " (4, 87),\n",
       " (4, 88),\n",
       " (4, 89),\n",
       " (4, 90),\n",
       " (4, 91),\n",
       " (4, 92),\n",
       " (4, 93),\n",
       " (4, 94),\n",
       " (4, 95),\n",
       " (4, 96),\n",
       " (4, 97),\n",
       " (4, 98),\n",
       " (5, 0),\n",
       " (5, 1),\n",
       " (5, 2),\n",
       " (5, 3),\n",
       " (5, 4),\n",
       " (5, 5),\n",
       " (5, 6),\n",
       " (5, 7),\n",
       " (5, 8),\n",
       " (5, 9),\n",
       " (5, 10),\n",
       " (5, 11),\n",
       " (5, 12),\n",
       " (5, 13),\n",
       " (5, 14),\n",
       " (5, 15),\n",
       " (5, 16),\n",
       " (5, 17),\n",
       " (5, 18),\n",
       " (5, 19),\n",
       " (5, 20),\n",
       " (5, 21),\n",
       " (5, 22),\n",
       " (5, 23),\n",
       " (5, 24),\n",
       " (5, 25),\n",
       " (5, 26),\n",
       " (5, 27),\n",
       " (5, 28),\n",
       " (5, 29),\n",
       " (5, 30),\n",
       " (5, 31),\n",
       " (5, 32),\n",
       " (5, 33),\n",
       " (5, 34),\n",
       " (5, 35),\n",
       " (5, 36),\n",
       " (5, 37),\n",
       " (5, 38),\n",
       " (5, 39),\n",
       " (5, 40),\n",
       " (5, 41),\n",
       " (5, 42),\n",
       " (5, 43),\n",
       " (5, 44),\n",
       " (5, 45),\n",
       " (5, 46),\n",
       " (5, 47),\n",
       " (5, 48),\n",
       " (5, 49),\n",
       " (5, 50),\n",
       " (5, 51),\n",
       " (5, 52),\n",
       " (5, 53),\n",
       " (5, 54),\n",
       " (5, 55),\n",
       " (5, 56),\n",
       " (5, 57),\n",
       " (5, 58),\n",
       " (5, 59),\n",
       " (5, 60),\n",
       " (5, 61),\n",
       " (5, 62),\n",
       " (5, 63),\n",
       " (5, 64),\n",
       " (5, 65),\n",
       " (5, 66),\n",
       " (5, 67),\n",
       " (5, 68),\n",
       " (5, 69),\n",
       " (5, 70),\n",
       " (5, 71),\n",
       " (5, 72),\n",
       " (5, 73),\n",
       " (5, 74),\n",
       " (5, 75),\n",
       " (5, 76),\n",
       " (5, 77),\n",
       " (5, 78),\n",
       " (5, 79),\n",
       " (5, 80),\n",
       " (5, 81),\n",
       " (5, 82),\n",
       " (5, 83),\n",
       " (5, 84),\n",
       " (5, 85),\n",
       " (5, 86),\n",
       " (5, 87),\n",
       " (5, 88),\n",
       " (5, 89),\n",
       " (5, 90),\n",
       " (5, 91),\n",
       " (5, 92),\n",
       " (5, 93),\n",
       " (5, 94),\n",
       " (5, 95),\n",
       " (5, 96),\n",
       " (5, 97),\n",
       " (5, 98),\n",
       " (6, 0),\n",
       " (6, 1),\n",
       " (6, 2),\n",
       " (6, 3),\n",
       " (6, 4),\n",
       " (6, 5),\n",
       " (6, 6),\n",
       " (6, 7),\n",
       " (6, 8),\n",
       " (6, 9),\n",
       " (6, 10),\n",
       " (6, 11),\n",
       " (6, 12),\n",
       " (6, 13),\n",
       " (6, 14),\n",
       " (6, 15),\n",
       " (6, 16),\n",
       " (6, 17),\n",
       " (6, 18),\n",
       " (6, 19),\n",
       " (6, 20),\n",
       " (6, 21),\n",
       " (6, 22),\n",
       " (6, 23),\n",
       " (6, 24),\n",
       " (6, 25),\n",
       " (6, 26),\n",
       " (6, 27),\n",
       " (6, 28),\n",
       " (6, 29),\n",
       " (6, 30),\n",
       " (6, 31),\n",
       " (6, 32),\n",
       " (6, 33),\n",
       " (6, 34),\n",
       " (6, 35),\n",
       " (6, 36),\n",
       " (6, 37),\n",
       " (6, 38),\n",
       " (6, 39),\n",
       " (6, 40),\n",
       " (6, 41),\n",
       " (6, 42),\n",
       " (6, 43),\n",
       " (6, 44),\n",
       " (6, 45),\n",
       " (6, 46),\n",
       " (6, 47),\n",
       " (6, 48),\n",
       " (6, 49),\n",
       " (6, 50),\n",
       " (6, 51),\n",
       " (6, 52),\n",
       " (6, 53),\n",
       " (6, 54),\n",
       " (6, 55),\n",
       " (6, 56),\n",
       " (6, 57),\n",
       " (6, 58),\n",
       " (6, 59),\n",
       " (6, 60),\n",
       " (6, 61),\n",
       " (6, 62),\n",
       " (6, 63),\n",
       " (6, 64),\n",
       " (6, 65),\n",
       " (6, 66),\n",
       " (6, 67),\n",
       " (6, 68),\n",
       " (6, 69),\n",
       " (6, 70),\n",
       " (6, 71),\n",
       " (6, 72),\n",
       " (6, 73),\n",
       " (6, 74),\n",
       " (6, 75),\n",
       " (6, 76),\n",
       " (6, 77),\n",
       " (6, 78),\n",
       " (6, 79),\n",
       " (6, 80),\n",
       " (6, 81),\n",
       " (6, 82),\n",
       " (6, 83),\n",
       " (6, 84),\n",
       " (6, 85),\n",
       " (6, 86),\n",
       " (6, 87),\n",
       " (6, 88),\n",
       " (6, 89),\n",
       " (6, 90),\n",
       " (6, 91),\n",
       " (6, 92),\n",
       " (6, 93),\n",
       " (6, 94),\n",
       " (6, 95),\n",
       " (6, 96),\n",
       " (6, 97),\n",
       " (6, 98),\n",
       " (7, 0),\n",
       " (7, 1),\n",
       " (7, 2),\n",
       " (7, 3),\n",
       " (7, 4),\n",
       " (7, 5),\n",
       " (7, 6),\n",
       " (7, 7),\n",
       " (7, 8),\n",
       " (7, 9),\n",
       " (7, 10),\n",
       " (7, 11),\n",
       " (7, 12),\n",
       " (7, 13),\n",
       " (7, 14),\n",
       " (7, 15),\n",
       " (7, 16),\n",
       " (7, 17),\n",
       " (7, 18),\n",
       " (7, 19),\n",
       " (7, 20),\n",
       " (7, 21),\n",
       " (7, 22),\n",
       " (7, 23),\n",
       " (7, 24),\n",
       " (7, 25),\n",
       " (7, 26),\n",
       " (7, 27),\n",
       " (7, 28),\n",
       " (7, 29),\n",
       " (7, 30),\n",
       " (7, 31),\n",
       " (7, 32),\n",
       " (7, 33),\n",
       " (7, 34),\n",
       " (7, 35),\n",
       " (7, 36),\n",
       " (7, 37),\n",
       " (7, 38),\n",
       " (7, 39),\n",
       " (7, 40),\n",
       " (7, 41),\n",
       " (7, 42),\n",
       " (7, 43),\n",
       " (7, 44),\n",
       " (7, 45),\n",
       " (7, 46),\n",
       " (7, 47),\n",
       " (7, 48),\n",
       " (7, 49),\n",
       " (7, 50),\n",
       " (7, 51),\n",
       " (7, 52),\n",
       " (7, 53),\n",
       " (7, 54),\n",
       " (7, 55),\n",
       " (7, 56),\n",
       " (7, 57),\n",
       " (7, 58),\n",
       " (7, 59),\n",
       " (7, 60),\n",
       " (7, 61),\n",
       " (7, 62),\n",
       " (7, 63),\n",
       " (7, 64),\n",
       " (7, 65),\n",
       " (7, 66),\n",
       " (7, 67),\n",
       " (7, 68),\n",
       " (7, 69),\n",
       " (7, 70),\n",
       " (7, 71),\n",
       " (7, 72),\n",
       " (7, 73),\n",
       " (7, 74),\n",
       " (7, 75),\n",
       " (7, 76),\n",
       " (7, 77),\n",
       " (7, 78),\n",
       " (7, 79),\n",
       " (7, 80),\n",
       " (7, 81),\n",
       " (7, 82),\n",
       " (7, 83),\n",
       " (7, 84),\n",
       " (7, 85),\n",
       " (7, 86),\n",
       " (7, 87),\n",
       " (7, 88),\n",
       " (7, 89),\n",
       " (7, 90),\n",
       " (7, 91),\n",
       " (7, 92),\n",
       " (7, 93),\n",
       " (7, 94),\n",
       " (7, 95),\n",
       " (7, 96),\n",
       " (7, 97),\n",
       " (7, 98),\n",
       " (8, 0),\n",
       " (8, 1),\n",
       " (8, 2),\n",
       " (8, 3),\n",
       " (8, 4),\n",
       " (8, 5),\n",
       " (8, 6),\n",
       " (8, 7),\n",
       " (8, 8),\n",
       " (8, 9),\n",
       " (8, 10),\n",
       " (8, 11),\n",
       " (8, 12),\n",
       " (8, 13),\n",
       " (8, 14),\n",
       " (8, 15),\n",
       " (8, 16),\n",
       " (8, 17),\n",
       " (8, 18),\n",
       " (8, 19),\n",
       " (8, 20),\n",
       " (8, 21),\n",
       " (8, 22),\n",
       " (8, 23),\n",
       " (8, 24),\n",
       " (8, 25),\n",
       " (8, 26),\n",
       " (8, 27),\n",
       " (8, 28),\n",
       " (8, 29),\n",
       " (8, 30),\n",
       " (8, 31),\n",
       " (8, 32),\n",
       " (8, 33),\n",
       " (8, 34),\n",
       " (8, 35),\n",
       " (8, 36),\n",
       " (8, 37),\n",
       " (8, 38),\n",
       " (8, 39),\n",
       " (8, 40),\n",
       " (8, 41),\n",
       " (8, 42),\n",
       " (8, 43),\n",
       " (8, 44),\n",
       " (8, 45),\n",
       " (8, 46),\n",
       " (8, 47),\n",
       " (8, 48),\n",
       " (8, 49),\n",
       " (8, 50),\n",
       " (8, 51),\n",
       " (8, 52),\n",
       " (8, 53),\n",
       " (8, 54),\n",
       " (8, 55),\n",
       " (8, 56),\n",
       " (8, 57),\n",
       " (8, 58),\n",
       " (8, 59),\n",
       " (8, 60),\n",
       " (8, 61),\n",
       " (8, 62),\n",
       " (8, 63),\n",
       " (8, 64),\n",
       " (8, 65),\n",
       " (8, 66),\n",
       " (8, 67),\n",
       " (8, 68),\n",
       " (8, 69),\n",
       " (8, 70),\n",
       " (8, 71),\n",
       " (8, 72),\n",
       " (8, 73),\n",
       " (8, 74),\n",
       " (8, 75),\n",
       " (8, 76),\n",
       " (8, 77),\n",
       " (8, 78),\n",
       " (8, 79),\n",
       " (8, 80),\n",
       " (8, 81),\n",
       " (8, 82),\n",
       " (8, 83),\n",
       " (8, 84),\n",
       " (8, 85),\n",
       " (8, 86),\n",
       " (8, 87),\n",
       " (8, 88),\n",
       " (8, 89),\n",
       " (8, 90),\n",
       " (8, 91),\n",
       " (8, 92),\n",
       " (8, 93),\n",
       " (8, 94),\n",
       " (8, 95),\n",
       " (8, 96),\n",
       " (8, 97),\n",
       " (8, 98),\n",
       " (9, 0),\n",
       " (9, 1),\n",
       " (9, 2),\n",
       " (9, 3),\n",
       " (9, 4),\n",
       " (9, 5),\n",
       " (9, 6),\n",
       " (9, 7),\n",
       " (9, 8),\n",
       " (9, 9),\n",
       " (9, 10),\n",
       " (9, 11),\n",
       " (9, 12),\n",
       " (9, 13),\n",
       " (9, 14),\n",
       " (9, 15),\n",
       " (9, 16),\n",
       " (9, 17),\n",
       " (9, 18),\n",
       " (9, 19),\n",
       " (9, 20),\n",
       " (9, 21),\n",
       " (9, 22),\n",
       " (9, 23),\n",
       " (9, 24),\n",
       " (9, 25),\n",
       " (9, 26),\n",
       " (9, 27),\n",
       " (9, 28),\n",
       " (9, 29),\n",
       " (9, 30),\n",
       " (9, 31),\n",
       " (9, 32),\n",
       " (9, 33),\n",
       " (9, 34),\n",
       " (9, 35),\n",
       " (9, 36),\n",
       " (9, 37),\n",
       " (9, 38),\n",
       " (9, 39),\n",
       " (9, 40),\n",
       " (9, 41),\n",
       " (9, 42),\n",
       " (9, 43),\n",
       " (9, 44),\n",
       " (9, 45),\n",
       " (9, 46),\n",
       " (9, 47),\n",
       " (9, 48),\n",
       " (9, 49),\n",
       " (9, 50),\n",
       " (9, 51),\n",
       " (9, 52),\n",
       " (9, 53),\n",
       " (9, 54),\n",
       " (9, 55),\n",
       " (9, 56),\n",
       " (9, 57),\n",
       " (9, 58),\n",
       " (9, 59),\n",
       " (9, 60),\n",
       " (9, 61),\n",
       " (9, 62),\n",
       " (9, 63),\n",
       " (9, 64),\n",
       " (9, 65),\n",
       " (9, 66),\n",
       " (9, 67),\n",
       " (9, 68),\n",
       " (9, 69),\n",
       " (9, 70),\n",
       " (9, 71),\n",
       " (9, 72),\n",
       " (9, 73),\n",
       " (9, 74),\n",
       " (9, 75),\n",
       " (9, 76),\n",
       " (9, 77),\n",
       " (9, 78),\n",
       " (9, 79),\n",
       " (9, 80),\n",
       " (9, 81),\n",
       " (9, 82),\n",
       " (9, 83),\n",
       " (9, 84),\n",
       " (9, 85),\n",
       " (9, 86),\n",
       " (9, 87),\n",
       " (9, 88),\n",
       " (9, 89),\n",
       " (9, 90),\n",
       " (9, 91),\n",
       " (9, 92),\n",
       " (9, 93),\n",
       " (9, 94),\n",
       " (9, 95),\n",
       " (9, 96),\n",
       " (9, 97),\n",
       " (9, 98),\n",
       " (10, 0),\n",
       " (10, 1),\n",
       " (10, 2),\n",
       " (10, 3),\n",
       " (10, 4),\n",
       " (10, 5),\n",
       " (10, 6),\n",
       " (10, 7),\n",
       " (10, 8),\n",
       " (10, 9),\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genes 1 99 (99, 12)\n",
      "Genes 2 99 (99, 12)\n"
     ]
    }
   ],
   "source": [
    "import CausalTests as ct\n",
    "\n",
    "found_genes, matr1 = gtm.get_gene_TS(data, data[\"gene\"])\n",
    "print \"Genes 1\", len(found_genes), matr1.shape\n",
    "\n",
    "found_genes, matr2 = gtm.get_gene_TS(rand_data, rand_data[\"gene\"])\n",
    "print \"Genes 2\", len(found_genes), matr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, b = ct.pairwise_granger_causality_matr_pair(matr2, matr1, pairs=pairlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = p.reshape((99, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = b.reshape((99, 99, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17231376,  0.02157578,  0.509251  , ...,  0.65101316,\n",
       "         0.55550772,  0.65147373],\n",
       "       [ 0.70524473,  0.90474361,  0.74811258, ...,  0.23967372,\n",
       "         0.76297546,  0.04013138],\n",
       "       [ 0.44836963,  0.58177713,  0.28416194, ...,  0.75931638,\n",
       "         0.91045006,  0.99259665],\n",
       "       ..., \n",
       "       [ 0.77832266,  0.9000753 ,  0.22030063, ...,  0.57236977,\n",
       "         0.34754995,  0.55980432],\n",
       "       [ 0.01062975,  0.94461404,  0.76677795, ...,  0.33633661,\n",
       "         0.10059922,  0.69913143],\n",
       "       [ 0.8582898 ,  0.5677802 ,  0.67071525, ...,  0.96745131,\n",
       "         0.06708298,  0.92990995]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17231376,  0.02157578,  0.509251  , ...,  0.65101316,\n",
       "         0.55550772,  0.65147373],\n",
       "       [ 0.70524473,  0.90474361,  0.74811258, ...,  0.23967372,\n",
       "         0.76297546,  0.04013138],\n",
       "       [ 0.44836963,  0.58177713,  0.28416194, ...,  0.75931638,\n",
       "         0.91045006,  0.99259665],\n",
       "       ..., \n",
       "       [ 0.77832266,  0.9000753 ,  0.22030063, ...,  0.57236977,\n",
       "         0.34754995,  0.55980432],\n",
       "       [ 0.01062975,  0.94461404,  0.76677795, ...,  0.33633661,\n",
       "         0.10059922,  0.69913143],\n",
       "       [ 0.8582898 ,  0.5677802 ,  0.67071525, ...,  0.96745131,\n",
       "         0.06708298,  0.92990995]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_p, test_b = pickle.load(open(\"della/small_TPM-test-pair/small_TPM-test-pair.p\", 'rB'))\n",
    "test_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.7587270769518e-11"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.absolute(p - test_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4827651873570176e-10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.absolute(b - test_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_p, norm_b = ct.pairwise_granger_causality_matr_pair(matr1, matr1, pairs=pairlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "norm_p = norm_p.reshape((99, 99))\n",
    "norm_b = norm_b.reshape((99, 99, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   1.23105640e-04,   2.80954308e-01, ...,\n",
       "          4.44145587e-01,   7.03103043e-01,   3.54487545e-01],\n",
       "       [  6.59124213e-01,   1.00000000e+00,   6.38674909e-01, ...,\n",
       "          7.77615718e-01,   4.64635296e-02,   4.98062905e-01],\n",
       "       [  7.09631185e-01,   8.81190719e-01,   1.00000000e+00, ...,\n",
       "          4.70088539e-01,   4.44184696e-01,   4.98996616e-01],\n",
       "       ..., \n",
       "       [  4.90948141e-01,   3.41208433e-01,   7.45665541e-01, ...,\n",
       "          1.00000000e+00,   1.99702334e-01,   1.61426502e-01],\n",
       "       [  7.18271966e-01,   1.11370851e-01,   8.19603777e-01, ...,\n",
       "          5.78737713e-01,   1.00000000e+00,   1.30014878e-01],\n",
       "       [  2.64311421e-01,   5.80215336e-01,   8.13349434e-01, ...,\n",
       "          4.64779226e-01,   1.33502706e-02,   1.00000000e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "res_df = pd.DataFrame(data={\"A\": [1,4,-1, 10], \"error.min\": [5,7,6, 20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = res_df.columns.values\n",
    "all_res_index = range(len(res_df) * len(rows))\n",
    "all_res_df = pd.DataFrame(index=all_res_index, columns=header)\n",
    "\n",
    "use_index = range(len(rows))\n",
    "use_df = pd.DataFrame(index=use_index, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, row in zip(range(len(rows)), rows):\n",
    "    res_df[\"error.min\"] = i * np.arange(len(rows))\n",
    "    index = i\n",
    "    all_res_df.iloc[len(res_df) * i : len(res_df) * (i + 1)] = res_df.as_matrix()\n",
    "    use_df.iloc[i] = res_df.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>error.min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A  error.min\n",
       "0   1          0\n",
       "1   4          1\n",
       "2  -1          4\n",
       "3  10          9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>error.min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A  error.min\n",
       "0    1          0\n",
       "1    4          0\n",
       "2   -1          0\n",
       "3   10          0\n",
       "4    1          0\n",
       "5    4          1\n",
       "6   -1          2\n",
       "7   10          3\n",
       "8    1          0\n",
       "9    4          2\n",
       "10  -1          4\n",
       "11  10          6\n",
       "12   1          0\n",
       "13   4          3\n",
       "14  -1          6\n",
       "15  10          9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(b_df[\"B\"] <= min(b_df[\"B\"]))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "0 -1  6\n",
       "1  1  5\n",
       "2  4  7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_df.index = range(len(b_df))\n",
    "b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_df.iloc[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.]\n",
      " [   0.    1.    2.    3.    4.    5.    6.    7.    8.    9.   10.   11.\n",
      "    12.   13.   14.   15.   16.   17.   18.   19.   20.   21.   22.   23.\n",
      "    24.   25.   26.   27.   28.   29.]\n",
      " [   0.    2.    4.    6.    8.   10.   12.   14.   16.   18.   20.   22.\n",
      "    24.   26.   28.   30.   32.   34.   36.   38.   40.   42.   44.   46.\n",
      "    48.   50.   52.   54.   56.   58.]\n",
      " [   0.    3.    6.    9.   12.   15.   18.   21.   24.   27.   30.   33.\n",
      "    36.   39.   42.   45.   48.   51.   54.   57.   60.   63.   66.   69.\n",
      "    72.   75.   78.   81.   84.   87.]\n",
      " [   0.    4.    8.   12.   16.   20.   24.   28.   32.   36.   40.   44.\n",
      "    48.   52.   56.   60.   64.   68.   72.   76.   80.   84.   88.   92.\n",
      "    96.  100.  104.  108.  112.  116.]\n",
      " [   0.    5.   10.   15.   20.   25.   30.   35.   40.   45.   50.   55.\n",
      "    60.   65.   70.   75.   80.   85.   90.   95.  100.  105.  110.  115.\n",
      "   120.  125.  130.  135.  140.  145.]\n",
      " [   0.    6.   12.   18.   24.   30.   36.   42.   48.   54.   60.   66.\n",
      "    72.   78.   84.   90.   96.  102.  108.  114.  120.  126.  132.  138.\n",
      "   144.  150.  156.  162.  168.  174.]\n",
      " [   0.    7.   14.   21.   28.   35.   42.   49.   56.   63.   70.   77.\n",
      "    84.   91.   98.  105.  112.  119.  126.  133.  140.  147.  154.  161.\n",
      "   168.  175.  182.  189.  196.  203.]\n",
      " [   0.    8.   16.   24.   32.   40.   48.   56.   64.   72.   80.   88.\n",
      "    96.  104.  112.  120.  128.  136.  144.  152.  160.  168.  176.  184.\n",
      "   192.  200.  208.  216.  224.  232.]\n",
      " [   0.    9.   18.   27.   36.   45.   54.   63.   72.   81.   90.   99.\n",
      "   108.  117.  126.  135.  144.  153.  162.  171.  180.  189.  198.  207.\n",
      "   216.  225.  234.  243.  252.  261.]\n",
      " [   0.   10.   20.   30.   40.   50.   60.   70.   80.   90.  100.  110.\n",
      "   120.  130.  140.  150.  160.  170.  180.  190.  200.  210.  220.  230.\n",
      "   240.  250.  260.  270.  280.  290.]\n",
      " [   0.   11.   22.   33.   44.   55.   66.   77.   88.   99.  110.  121.\n",
      "   132.  143.  154.  165.  176.  187.  198.  209.  220.  231.  242.  253.\n",
      "   264.  275.  286.  297.  308.  319.]]\n",
      "[  0  -1  -2  -3  -4  -5  -6  -7  -8  -9 -10 -11]\n"
     ]
    }
   ],
   "source": [
    "T = 12\n",
    "lag = 2\n",
    "n = 30\n",
    "X = np.zeros((T, n))\n",
    "\n",
    "for i in range(T):\n",
    "    X[i] = np.arange(n) * i\n",
    "print X\n",
    "\n",
    "y = np.arange(T) * -1 \n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_t = np.zeros((T - lag, n * lag))\n",
    "\n",
    "for j in range(T - lag):\n",
    "    for k in range(lag):\n",
    "        X_t[j, n*k:n*(k+1)] = X[j + lag - k, ]\n",
    "\n",
    "y_t = y[(lag+1):T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]\n",
      "[ 1  2  3  5  6  7  9 10 11] [0 4 8]\n",
      "[ 0  2  3  4  6  7  8 10 11] [1 5 9]\n",
      "[ 0  1  3  4  5  7  8  9 11] [ 2  6 10]\n",
      "[ 0  1  2  4  5  6  8  9 10] [ 3  7 11]\n"
     ]
    }
   ],
   "source": [
    "labels = [i /4 for i in range(12)]\n",
    "print labels\n",
    "cv = StratifiedKFold(labels, 4)\n",
    "for train_index, test_index in cv:\n",
    "    print train_index, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4, 3, 5, 0, 6], array([1, 2, 3, 3, 5]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_min_k(d, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import CausalTests as ct\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = 60, 100\n",
    "\n",
    "np.random.seed(1)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "coef = 3*np.random.randn(n_features)\n",
    "coef[10:] = 0 # sparsify coef\n",
    "y = np.dot(X, coef)\n",
    "\n",
    "# add noise\n",
    "y += 0.01 * np.random.normal((n_samples,))\n",
    "\n",
    "# Split data in train set and test se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = n_features\n",
    "T = n_samples\n",
    "lag = 2\n",
    "\n",
    "X_t = np.zeros((T - lag, n * lag))\n",
    "\n",
    "for j in range(T - lag):\n",
    "    for k in range(lag):\n",
    "        X_t[j, n*k:n*(k+1)] = X[j + lag - k, ]\n",
    "\n",
    "y_t = y[lag:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01  0.02  0.03  0.04  0.05  0.06  0.07  0.08  0.09  0.1   0.11  0.12\n",
      "  0.13  0.14  0.15  0.16  0.17  0.18  0.19  0.2   0.21  0.22  0.23  0.24\n",
      "  0.25  0.26  0.27  0.28  0.29  0.3   0.31  0.32  0.33  0.34  0.35  0.36\n",
      "  0.37  0.38  0.39  0.4   0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48\n",
      "  0.49  0.5   0.51  0.52  0.53  0.54  0.55  0.56  0.57  0.58  0.59  0.6\n",
      "  0.61  0.62  0.63  0.64  0.65  0.66  0.67  0.68  0.69  0.7   0.71  0.72\n",
      "  0.73  0.74  0.75  0.76  0.77  0.78  0.79  0.8   0.81  0.82  0.83  0.84\n",
      "  0.85  0.86  0.87  0.88  0.89  0.9   0.91  0.92  0.93  0.94  0.95  0.96\n",
      "  0.97  0.98  0.99]\n"
     ]
    }
   ],
   "source": [
    "top_num = 3\n",
    "n_folds = 3\n",
    "max_iter = 100\n",
    "print alphas\n",
    "\n",
    "cv = ct.get_TS_cv(len(y_t), n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNetCV\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'CausalTests' from 'CausalTests.py'>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For # alphas:  300\n",
      "CPU times: user 17.1 s, sys: 104 ms, total: 17.2 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "cv = ct.get_TS_cv(len(y_t), n_folds)\n",
    "\n",
    "print \"For # alphas: \", len(alphas)\n",
    "%time top_df = ct.enet_granger_causality_cv(X_t, y_t, cv, alphas=alphas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>error.min</th>\n",
       "      <th>lambda.min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.99</td>\n",
       "      <td>22.534364</td>\n",
       "      <td>0.007259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.99</td>\n",
       "      <td>22.549312</td>\n",
       "      <td>0.007784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.99</td>\n",
       "      <td>22.566005</td>\n",
       "      <td>0.008347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.99</td>\n",
       "      <td>22.582873</td>\n",
       "      <td>0.008950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.99</td>\n",
       "      <td>22.600528</td>\n",
       "      <td>0.009596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  error.min  lambda.min\n",
       "0   0.99  22.534364    0.007259\n",
       "1   0.99  22.549312    0.007784\n",
       "2   0.99  22.566005    0.008347\n",
       "3   0.99  22.582873    0.008950\n",
       "4   0.99  22.600528    0.009596"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print top_df.shape\n",
    "top_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_t + 1\n",
    "np.average((y_t - y_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mses = enet.mse_path_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this will keep the smallest\n",
    "top_indices, top_mses = ct.get_min_k(cv_mses, top_num)\n",
    "\n",
    "top_lambdas = cv_lambdas[top_indices]\n",
    "top_alphas = cv_alphas[top_indices]\n",
    "\n",
    "top_df = pd.DataFrame(data={\"lambda.min\": top_lambdas, \"alpha\": top_alphas, \"error.min\": top_mses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.666666666666667"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([5,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 200)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha  error.min  lambda.min  test_err     score    df\n",
      "0    0.99  22.534364    0.007259  0.000711  0.999993  17.0\n",
      "1    0.99  22.549312    0.007784  0.000818  0.999992  17.0\n",
      "2    0.99  22.566005    0.008347  0.000920  0.999991  17.0\n",
      "3    0.99  22.582873    0.008950  0.001078  0.999990  17.0\n",
      "4    0.99  22.600528    0.009596  0.001207  0.999989  17.0\n",
      "5    0.99  22.617994    0.010290  0.001391  0.999987  17.0\n",
      "6    0.99  22.634447    0.011034  0.001587  0.999985  17.0\n",
      "7    0.99  22.650516    0.011831  0.001839  0.999983  17.0\n",
      "8    0.99  22.667412    0.012686  0.002110  0.999980  17.0\n",
      "9    0.99  22.692117    0.013603  0.002426  0.999977  17.0\n",
      "10   0.99  22.710110    0.014586  0.002767  0.999974  17.0\n",
      "11   0.99  22.735867    0.015640  0.003193  0.999970  17.0\n",
      "12   0.99  22.753928    0.016770  0.003657  0.999966  17.0\n",
      "13   0.99  22.770854    0.017982  0.004197  0.999961  17.0\n",
      "14   0.99  22.787013    0.019282  0.004856  0.999955  17.0\n",
      "15   0.99  22.802714    0.020675  0.005579  0.999948  17.0\n",
      "16   0.99  22.819410    0.022169  0.006373  0.999941  17.0\n",
      "17   0.99  22.837379    0.023771  0.007335  0.999932  17.0\n",
      "18   0.99  22.855607    0.025489  0.008449  0.999921  17.0\n",
      "19   0.99  22.877704    0.027331  0.009679  0.999910  17.0\n",
      "20   0.99  22.901159    0.029306  0.011161  0.999896  17.0\n",
      "21   0.99  22.924953    0.031424  0.012800  0.999881  17.0\n",
      "22   0.99  22.945622    0.033695  0.014695  0.999863  17.0\n",
      "23   0.99  22.958765    0.036130  0.016881  0.999843  17.0\n",
      "24   0.99  22.969875    0.038741  0.019396  0.999819  17.0\n",
      "25   0.99  22.990593    0.041541  0.022291  0.999792  17.0\n",
      "26   0.98  22.992477    0.007333  0.000777  0.999993  20.0\n",
      "27   0.98  23.001730    0.007863  0.000875  0.999992  20.0\n",
      "28   0.98  23.011145    0.008432  0.001008  0.999991  20.0\n",
      "29   0.99  23.019577    0.044543  0.025622  0.999761  17.0\n",
      "..    ...        ...         ...       ...       ...   ...\n",
      "70   0.99  23.856275    0.118310  0.179369  0.998327  17.0\n",
      "71   0.98  23.885821    0.059484  0.047743  0.999555  17.0\n",
      "72   0.98  23.913898    0.063783  0.054896  0.999488  17.0\n",
      "73   0.98  24.026466    0.068392  0.062975  0.999413  17.0\n",
      "74   0.99  24.041721    0.126859  0.206092  0.998078  17.0\n",
      "75   0.98  24.146074    0.073334  0.072391  0.999325  17.0\n",
      "76   0.98  24.277994    0.078634  0.083226  0.999224  17.0\n",
      "77   0.99  24.295825    0.136027  0.236824  0.997791  17.0\n",
      "78   0.98  24.422122    0.084317  0.095529  0.999109  17.0\n",
      "79   0.98  24.570398    0.090410  0.109797  0.998976  17.0\n",
      "80   0.99  24.597146    0.145858  0.272064  0.997463  17.0\n",
      "81   0.97  24.658190    0.007409  0.000837  0.999992  22.0\n",
      "82   0.97  24.664001    0.007944  0.000955  0.999991  22.0\n",
      "83   0.97  24.670293    0.008519  0.001097  0.999990  22.0\n",
      "84   0.97  24.674144    0.009134  0.001248  0.999988  22.0\n",
      "85   0.97  24.684350    0.009794  0.001426  0.999987  21.0\n",
      "86   0.97  24.689987    0.010502  0.001635  0.999985  22.0\n",
      "87   0.97  24.699314    0.011261  0.001882  0.999982  22.0\n",
      "88   0.97  24.708258    0.012075  0.002143  0.999980  21.0\n",
      "89   0.97  24.716476    0.012948  0.002450  0.999977  21.0\n",
      "90   0.97  24.724619    0.013883  0.002810  0.999974  21.0\n",
      "91   0.98  24.735464    0.096944  0.126191  0.998823  17.0\n",
      "92   0.97  24.738447    0.014886  0.003229  0.999970  21.0\n",
      "93   0.97  24.748106    0.015962  0.003686  0.999966  21.0\n",
      "94   0.97  24.764567    0.017116  0.004248  0.999960  21.0\n",
      "95   0.97  24.778298    0.018353  0.004864  0.999955  21.0\n",
      "96   0.97  24.800893    0.019679  0.005578  0.999948  21.0\n",
      "97   0.97  24.816449    0.021101  0.006403  0.999940  21.0\n",
      "98   0.97  24.838138    0.022626  0.007356  0.999931  21.0\n",
      "99   0.99  24.856555    0.156398  0.312616  0.997085  17.0\n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "test_errs = np.zeros(len(top_df))\n",
    "scores = np.zeros(len(top_df))\n",
    "dfs = np.zeros(len(top_df))\n",
    "\n",
    "test_coefs = np.zeros((len(top_df), X_t.shape[1]))\n",
    "for i in range(len(top_df)):\n",
    "    alpha = top_df.iloc[i][\"alpha\"]\n",
    "    lambda_min = top_df.iloc[i][\"lambda.min\"]\n",
    "    enet = ElasticNet(l1_ratio=alpha, alpha=lambda_min)\n",
    "    enet.fit(X_t, y_t)\n",
    "    y_pred = enet.predict(X_t)\n",
    "    test_errs[i] = np.average((y_t - y_pred)**2)\n",
    "    scores[i] = enet.score(X_t, y_t)\n",
    "    test_coefs[i] = enet.coef_\n",
    "    \n",
    "    dfs[i] = len(np.where(enet.coef_)[0])\n",
    "    \n",
    "top_df[\"test_err\"] = test_errs\n",
    "top_df[\"score\"] = scores\n",
    "top_df[\"df\"] = dfs\n",
    "\n",
    "print top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'CausalTests' from 'CausalTests.py'>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_df, test_coefs = ct.enet_granger_causality_test(X_t, y_t, top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 200)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>error.min</th>\n",
       "      <th>lambda.min</th>\n",
       "      <th>test_err</th>\n",
       "      <th>score</th>\n",
       "      <th>df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.959887</td>\n",
       "      <td>30.555324</td>\n",
       "      <td>0.114722</td>\n",
       "      <td>0.403671</td>\n",
       "      <td>0.996236</td>\n",
       "      <td>22.792500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.018756</td>\n",
       "      <td>5.757457</td>\n",
       "      <td>0.127802</td>\n",
       "      <td>0.741618</td>\n",
       "      <td>0.006916</td>\n",
       "      <td>5.047839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.927726</td>\n",
       "      <td>22.534364</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.961815</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.944114</td>\n",
       "      <td>25.278864</td>\n",
       "      <td>0.020650</td>\n",
       "      <td>0.006540</td>\n",
       "      <td>0.996186</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.960502</td>\n",
       "      <td>29.768741</td>\n",
       "      <td>0.058742</td>\n",
       "      <td>0.051678</td>\n",
       "      <td>0.999518</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.976890</td>\n",
       "      <td>35.522659</td>\n",
       "      <td>0.167840</td>\n",
       "      <td>0.408972</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>41.564513</td>\n",
       "      <td>0.592755</td>\n",
       "      <td>4.094693</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             alpha    error.min   lambda.min     test_err        score  \\\n",
       "count  1200.000000  1200.000000  1200.000000  1200.000000  1200.000000   \n",
       "mean      0.959887    30.555324     0.114722     0.403671     0.996236   \n",
       "std       0.018756     5.757457     0.127802     0.741618     0.006916   \n",
       "min       0.927726    22.534364     0.007259     0.000711     0.961815   \n",
       "25%       0.944114    25.278864     0.020650     0.006540     0.996186   \n",
       "50%       0.960502    29.768741     0.058742     0.051678     0.999518   \n",
       "75%       0.976890    35.522659     0.167840     0.408972     0.999939   \n",
       "max       0.990000    41.564513     0.592755     4.094693     0.999993   \n",
       "\n",
       "                df  \n",
       "count  1200.000000  \n",
       "mean     22.792500  \n",
       "std       5.047839  \n",
       "min      14.000000  \n",
       "25%      18.000000  \n",
       "50%      23.000000  \n",
       "75%      28.000000  \n",
       "max      33.000000  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>error.min</th>\n",
       "      <th>lambda.min</th>\n",
       "      <th>test_err</th>\n",
       "      <th>score</th>\n",
       "      <th>df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>22.534364</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.986722</td>\n",
       "      <td>22.725284</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.983445</td>\n",
       "      <td>22.854051</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.980167</td>\n",
       "      <td>22.984007</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.976890</td>\n",
       "      <td>23.420661</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.973612</td>\n",
       "      <td>24.029752</td>\n",
       "      <td>0.007382</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>22.549312</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.970334</td>\n",
       "      <td>24.622360</td>\n",
       "      <td>0.007406</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.986722</td>\n",
       "      <td>22.738633</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.967057</td>\n",
       "      <td>24.986905</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.983445</td>\n",
       "      <td>22.865538</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.963779</td>\n",
       "      <td>25.370603</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.980167</td>\n",
       "      <td>22.992462</td>\n",
       "      <td>0.007862</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.976890</td>\n",
       "      <td>23.431692</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.960502</td>\n",
       "      <td>26.169731</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0.957224</td>\n",
       "      <td>27.486365</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>22.566005</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.973612</td>\n",
       "      <td>24.042805</td>\n",
       "      <td>0.007915</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.986722</td>\n",
       "      <td>22.751837</td>\n",
       "      <td>0.008374</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.970334</td>\n",
       "      <td>24.628326</td>\n",
       "      <td>0.007942</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>0.953946</td>\n",
       "      <td>28.740207</td>\n",
       "      <td>0.007534</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>0.950669</td>\n",
       "      <td>29.773077</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.967057</td>\n",
       "      <td>24.990488</td>\n",
       "      <td>0.007969</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.983445</td>\n",
       "      <td>22.876342</td>\n",
       "      <td>0.008402</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>0.947391</td>\n",
       "      <td>30.790470</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.980167</td>\n",
       "      <td>23.000991</td>\n",
       "      <td>0.008430</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0.963779</td>\n",
       "      <td>25.374615</td>\n",
       "      <td>0.007996</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.960502</td>\n",
       "      <td>26.173595</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>0.944114</td>\n",
       "      <td>32.077106</td>\n",
       "      <td>0.007612</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.976890</td>\n",
       "      <td>23.439622</td>\n",
       "      <td>0.008459</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>35.425136</td>\n",
       "      <td>0.477618</td>\n",
       "      <td>2.825755</td>\n",
       "      <td>0.973648</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>0.957224</td>\n",
       "      <td>40.168402</td>\n",
       "      <td>0.460680</td>\n",
       "      <td>2.861632</td>\n",
       "      <td>0.973314</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.986722</td>\n",
       "      <td>35.950988</td>\n",
       "      <td>0.479204</td>\n",
       "      <td>2.863121</td>\n",
       "      <td>0.973300</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>0.953946</td>\n",
       "      <td>40.807659</td>\n",
       "      <td>0.462263</td>\n",
       "      <td>2.899078</td>\n",
       "      <td>0.972965</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>0.983445</td>\n",
       "      <td>36.481709</td>\n",
       "      <td>0.480801</td>\n",
       "      <td>2.901127</td>\n",
       "      <td>0.972946</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>0.950669</td>\n",
       "      <td>41.474315</td>\n",
       "      <td>0.463856</td>\n",
       "      <td>2.935969</td>\n",
       "      <td>0.972621</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.980167</td>\n",
       "      <td>37.032074</td>\n",
       "      <td>0.482409</td>\n",
       "      <td>2.939766</td>\n",
       "      <td>0.972585</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>0.976890</td>\n",
       "      <td>37.627521</td>\n",
       "      <td>0.484028</td>\n",
       "      <td>2.979041</td>\n",
       "      <td>0.972219</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>0.973612</td>\n",
       "      <td>38.269577</td>\n",
       "      <td>0.485657</td>\n",
       "      <td>3.018543</td>\n",
       "      <td>0.971851</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>0.970334</td>\n",
       "      <td>38.919034</td>\n",
       "      <td>0.487298</td>\n",
       "      <td>3.057916</td>\n",
       "      <td>0.971483</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>0.967057</td>\n",
       "      <td>39.595949</td>\n",
       "      <td>0.488949</td>\n",
       "      <td>3.097710</td>\n",
       "      <td>0.971112</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>0.963779</td>\n",
       "      <td>40.269287</td>\n",
       "      <td>0.490612</td>\n",
       "      <td>3.138159</td>\n",
       "      <td>0.970735</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>36.752457</td>\n",
       "      <td>0.512134</td>\n",
       "      <td>3.178822</td>\n",
       "      <td>0.970356</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>0.960502</td>\n",
       "      <td>40.937284</td>\n",
       "      <td>0.492286</td>\n",
       "      <td>3.179385</td>\n",
       "      <td>0.970351</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.986722</td>\n",
       "      <td>37.311783</td>\n",
       "      <td>0.513835</td>\n",
       "      <td>3.220848</td>\n",
       "      <td>0.969964</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>0.983445</td>\n",
       "      <td>37.890135</td>\n",
       "      <td>0.515548</td>\n",
       "      <td>3.263572</td>\n",
       "      <td>0.969566</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>0.980167</td>\n",
       "      <td>38.468792</td>\n",
       "      <td>0.517272</td>\n",
       "      <td>3.306990</td>\n",
       "      <td>0.969161</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>0.976890</td>\n",
       "      <td>39.045533</td>\n",
       "      <td>0.519007</td>\n",
       "      <td>3.351096</td>\n",
       "      <td>0.968749</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>0.973612</td>\n",
       "      <td>39.686749</td>\n",
       "      <td>0.520754</td>\n",
       "      <td>3.395894</td>\n",
       "      <td>0.968332</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>0.970334</td>\n",
       "      <td>40.375357</td>\n",
       "      <td>0.522513</td>\n",
       "      <td>3.441416</td>\n",
       "      <td>0.967907</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>0.967057</td>\n",
       "      <td>41.081074</td>\n",
       "      <td>0.524284</td>\n",
       "      <td>3.484486</td>\n",
       "      <td>0.967505</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>38.321802</td>\n",
       "      <td>0.549144</td>\n",
       "      <td>3.571733</td>\n",
       "      <td>0.966692</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>0.986722</td>\n",
       "      <td>38.924413</td>\n",
       "      <td>0.550969</td>\n",
       "      <td>3.614942</td>\n",
       "      <td>0.966289</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>0.983445</td>\n",
       "      <td>39.521651</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>3.658941</td>\n",
       "      <td>0.965879</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>0.980167</td>\n",
       "      <td>40.114316</td>\n",
       "      <td>0.554653</td>\n",
       "      <td>3.703748</td>\n",
       "      <td>0.965461</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>0.976890</td>\n",
       "      <td>40.709652</td>\n",
       "      <td>0.556514</td>\n",
       "      <td>3.749344</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>0.973612</td>\n",
       "      <td>41.328305</td>\n",
       "      <td>0.558388</td>\n",
       "      <td>3.795748</td>\n",
       "      <td>0.964603</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>40.143323</td>\n",
       "      <td>0.588830</td>\n",
       "      <td>3.997142</td>\n",
       "      <td>0.962725</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>0.986722</td>\n",
       "      <td>40.674727</td>\n",
       "      <td>0.590786</td>\n",
       "      <td>4.045197</td>\n",
       "      <td>0.962277</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>0.983445</td>\n",
       "      <td>41.214922</td>\n",
       "      <td>0.592755</td>\n",
       "      <td>4.094693</td>\n",
       "      <td>0.961815</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         alpha  error.min  lambda.min  test_err     score    df\n",
       "0     0.990000  22.534364    0.007259  0.000711  0.999993  17.0\n",
       "11    0.986722  22.725284    0.007283  0.000734  0.999993  19.0\n",
       "28    0.983445  22.854051    0.007308  0.000755  0.999993  20.0\n",
       "54    0.980167  22.984007    0.007332  0.000775  0.999993  20.0\n",
       "116   0.976890  23.420661    0.007357  0.000800  0.999993  20.0\n",
       "176   0.973612  24.029752    0.007382  0.000808  0.999992  21.0\n",
       "1     0.990000  22.549312    0.007784  0.000818  0.999992  17.0\n",
       "227   0.970334  24.622360    0.007406  0.000833  0.999992  22.0\n",
       "13    0.986722  22.738633    0.007810  0.000842  0.999992  19.0\n",
       "262   0.967057  24.986905    0.007432  0.000859  0.999992  23.0\n",
       "31    0.983445  22.865538    0.007836  0.000866  0.999992  20.0\n",
       "305   0.963779  25.370603    0.007457  0.000871  0.999992  24.0\n",
       "56    0.980167  22.992462    0.007862  0.000888  0.999992  20.0\n",
       "118   0.976890  23.431692    0.007888  0.000896  0.999992  20.0\n",
       "374   0.960502  26.169731    0.007482  0.000900  0.999992  25.0\n",
       "454   0.957224  27.486365    0.007508  0.000917  0.999991  26.0\n",
       "2     0.990000  22.566005    0.008347  0.000920  0.999991  17.0\n",
       "179   0.973612  24.042805    0.007915  0.000923  0.999991  20.0\n",
       "14    0.986722  22.751837    0.008374  0.000948  0.999991  17.0\n",
       "228   0.970334  24.628326    0.007942  0.000951  0.999991  22.0\n",
       "527   0.953946  28.740207    0.007534  0.000953  0.999991  26.0\n",
       "606   0.950669  29.773077    0.007560  0.000974  0.999991  28.0\n",
       "263   0.967057  24.990488    0.007969  0.000979  0.999991  23.0\n",
       "32    0.983445  22.876342    0.008402  0.000980  0.999991  19.0\n",
       "650   0.947391  30.790470    0.007586  0.000998  0.999991  29.0\n",
       "58    0.980167  23.000991    0.008430  0.001006  0.999991  20.0\n",
       "306   0.963779  25.374615    0.007996  0.001008  0.999991  24.0\n",
       "376   0.960502  26.173595    0.008023  0.001023  0.999990  25.0\n",
       "717   0.944114  32.077106    0.007612  0.001024  0.999990  31.0\n",
       "121   0.976890  23.439622    0.008459  0.001031  0.999990  20.0\n",
       "...        ...        ...         ...       ...       ...   ...\n",
       "891   0.990000  35.425136    0.477618  2.825755  0.973648  14.0\n",
       "1125  0.957224  40.168402    0.460680  2.861632  0.973314  18.0\n",
       "924   0.986722  35.950988    0.479204  2.863121  0.973300  14.0\n",
       "1153  0.953946  40.807659    0.462263  2.899078  0.972965  19.0\n",
       "946   0.983445  36.481709    0.480801  2.901127  0.972946  14.0\n",
       "1195  0.950669  41.474315    0.463856  2.935969  0.972621  19.0\n",
       "980   0.980167  37.032074    0.482409  2.939766  0.972585  14.0\n",
       "1006  0.976890  37.627521    0.484028  2.979041  0.972219  14.0\n",
       "1036  0.973612  38.269577    0.485657  3.018543  0.971851  15.0\n",
       "1072  0.970334  38.919034    0.487298  3.057916  0.971483  16.0\n",
       "1106  0.967057  39.595949    0.488949  3.097710  0.971112  16.0\n",
       "1127  0.963779  40.269287    0.490612  3.138159  0.970735  16.0\n",
       "957   0.990000  36.752457    0.512134  3.178822  0.970356  14.0\n",
       "1160  0.960502  40.937284    0.492286  3.179385  0.970351  17.0\n",
       "997   0.986722  37.311783    0.513835  3.220848  0.969964  14.0\n",
       "1025  0.983445  37.890135    0.515548  3.263572  0.969566  14.0\n",
       "1043  0.980167  38.468792    0.517272  3.306990  0.969161  14.0\n",
       "1079  0.976890  39.045533    0.519007  3.351096  0.968749  14.0\n",
       "1114  0.973612  39.686749    0.520754  3.395894  0.968332  14.0\n",
       "1130  0.970334  40.375357    0.522513  3.441416  0.967907  15.0\n",
       "1170  0.967057  41.081074    0.524284  3.484486  0.967505  14.0\n",
       "1040  0.990000  38.321802    0.549144  3.571733  0.966692  14.0\n",
       "1073  0.986722  38.924413    0.550969  3.614942  0.966289  14.0\n",
       "1097  0.983445  39.521651    0.552805  3.658941  0.965879  14.0\n",
       "1123  0.980167  40.114316    0.554653  3.703748  0.965461  14.0\n",
       "1141  0.976890  40.709652    0.556514  3.749344  0.965035  14.0\n",
       "1190  0.973612  41.328305    0.558388  3.795748  0.964603  14.0\n",
       "1124  0.990000  40.143323    0.588830  3.997142  0.962725  14.0\n",
       "1134  0.986722  40.674727    0.590786  4.045197  0.962277  14.0\n",
       "1184  0.983445  41.214922    0.592755  4.094693  0.961815  14.0\n",
       "\n",
       "[1200 rows x 6 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_df.sort(\"test_err\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geneTSmunging as gtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small data, 100 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg + STD loaded\n",
      "(100, 12)\n",
      "[[ 0.49484238  0.53714282  0.62506496 ...,  0.52527729  0.53656133\n",
      "   0.35290245]\n",
      " [ 0.95042984  0.85225909  0.77666868 ...,  1.00415122  0.77036824\n",
      "   0.98011805]\n",
      " [ 3.54428738  3.55911824  3.10624164 ...,  3.31815621  3.15048125\n",
      "   3.14930872]\n",
      " ..., \n",
      " [ 1.98618627  1.9497701   1.81032042 ...,  1.67106721  1.6633224\n",
      "   1.56155976]\n",
      " [ 5.89298407  5.85591087  5.88054204 ...,  5.92054549  5.94998633\n",
      "   5.97521896]\n",
      " [ 1.51217844  1.45703843  1.51720832 ...,  1.44695417  1.38895757\n",
      "   1.51810804]]\n"
     ]
    }
   ],
   "source": [
    "data = gtm.load_file_and_avg(\"Enet/small_TPM-avg.txt\")\n",
    "\n",
    "found_genes, geneTS = gtm.get_gene_TS(data, data[\"gene\"])\n",
    "\n",
    "print geneTS.shape\n",
    "print geneTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01        0.01245614  0.01491228  0.01736842  0.01982456  0.0222807\n",
      "  0.02473684  0.02719298  0.02964912  0.03210526  0.0345614   0.03701754\n",
      "  0.03947368  0.04192982  0.04438596  0.04684211  0.04929825  0.05175439\n",
      "  0.05421053  0.05666667  0.05912281  0.06157895  0.06403509  0.06649123\n",
      "  0.06894737  0.07140351  0.07385965  0.07631579  0.07877193  0.08122807\n",
      "  0.08368421  0.08614035  0.08859649  0.09105263  0.09350877  0.09596491\n",
      "  0.09842105  0.10087719  0.10333333  0.10578947  0.10824561  0.11070175\n",
      "  0.11315789  0.11561404  0.11807018  0.12052632  0.12298246  0.1254386\n",
      "  0.12789474  0.13035088  0.13280702  0.13526316  0.1377193   0.14017544\n",
      "  0.14263158  0.14508772  0.14754386  0.15        0.15245614  0.15491228\n",
      "  0.15736842  0.15982456  0.1622807   0.16473684  0.16719298  0.16964912\n",
      "  0.17210526  0.1745614   0.17701754  0.17947368  0.18192982  0.18438596\n",
      "  0.18684211  0.18929825  0.19175439  0.19421053  0.19666667  0.19912281\n",
      "  0.20157895  0.20403509  0.20649123  0.20894737  0.21140351  0.21385965\n",
      "  0.21631579  0.21877193  0.22122807  0.22368421  0.22614035  0.22859649\n",
      "  0.23105263  0.23350877  0.23596491  0.23842105  0.24087719  0.24333333\n",
      "  0.24578947  0.24824561  0.25070175  0.25315789  0.25561404  0.25807018\n",
      "  0.26052632  0.26298246  0.2654386   0.26789474  0.27035088  0.27280702\n",
      "  0.27526316  0.2777193   0.28017544  0.28263158  0.28508772  0.28754386\n",
      "  0.29        0.29245614  0.29491228  0.29736842  0.29982456  0.3022807\n",
      "  0.30473684  0.30719298  0.30964912  0.31210526  0.3145614   0.31701754\n",
      "  0.31947368  0.32192982  0.32438596  0.32684211  0.32929825  0.33175439\n",
      "  0.33421053  0.33666667  0.33912281  0.34157895  0.34403509  0.34649123\n",
      "  0.34894737  0.35140351  0.35385965  0.35631579  0.35877193  0.36122807\n",
      "  0.36368421  0.36614035  0.36859649  0.37105263  0.37350877  0.37596491\n",
      "  0.37842105  0.38087719  0.38333333  0.38578947  0.38824561  0.39070175\n",
      "  0.39315789  0.39561404  0.39807018  0.40052632  0.40298246  0.4054386\n",
      "  0.40789474  0.41035088  0.41280702  0.41526316  0.4177193   0.42017544\n",
      "  0.42263158  0.42508772  0.42754386  0.43        0.43245614  0.43491228\n",
      "  0.43736842  0.43982456  0.4422807   0.44473684  0.44719298  0.44964912\n",
      "  0.45210526  0.4545614   0.45701754  0.45947368  0.46192982  0.46438596\n",
      "  0.46684211  0.46929825  0.47175439  0.47421053  0.47666667  0.47912281\n",
      "  0.48157895  0.48403509  0.48649123  0.48894737  0.49140351  0.49385965\n",
      "  0.49631579  0.49877193  0.50122807  0.50368421  0.50614035  0.50859649\n",
      "  0.51105263  0.51350877  0.51596491  0.51842105  0.52087719  0.52333333\n",
      "  0.52578947  0.52824561  0.53070175  0.53315789  0.53561404  0.53807018\n",
      "  0.54052632  0.54298246  0.5454386   0.54789474  0.55035088  0.55280702\n",
      "  0.55526316  0.5577193   0.56017544  0.56263158  0.56508772  0.56754386\n",
      "  0.57        0.57245614  0.57491228  0.57736842  0.57982456  0.5822807\n",
      "  0.58473684  0.58719298  0.58964912  0.59210526  0.5945614   0.59701754\n",
      "  0.59947368  0.60192982  0.60438596  0.60684211  0.60929825  0.61175439\n",
      "  0.61421053  0.61666667  0.61912281  0.62157895  0.62403509  0.62649123\n",
      "  0.62894737  0.63140351  0.63385965  0.63631579  0.63877193  0.64122807\n",
      "  0.64368421  0.64614035  0.64859649  0.65105263  0.65350877  0.65596491\n",
      "  0.65842105  0.66087719  0.66333333  0.66578947  0.66824561  0.67070175\n",
      "  0.67315789  0.67561404  0.67807018  0.68052632  0.68298246  0.6854386\n",
      "  0.68789474  0.69035088  0.69280702  0.69526316  0.6977193   0.70017544\n",
      "  0.70263158  0.70508772  0.70754386  0.71        0.71245614  0.71491228\n",
      "  0.71736842  0.71982456  0.7222807   0.72473684  0.72719298  0.72964912\n",
      "  0.73210526  0.7345614   0.73701754  0.73947368  0.74192982  0.74438596\n",
      "  0.74684211  0.74929825  0.75175439  0.75421053  0.75666667  0.75912281\n",
      "  0.76157895  0.76403509  0.76649123  0.76894737  0.77140351  0.77385965\n",
      "  0.77631579  0.77877193  0.78122807  0.78368421  0.78614035  0.78859649\n",
      "  0.79105263  0.79350877  0.79596491  0.79842105  0.80087719  0.80333333\n",
      "  0.80578947  0.80824561  0.81070175  0.81315789  0.81561404  0.81807018\n",
      "  0.82052632  0.82298246  0.8254386   0.82789474  0.83035088  0.83280702\n",
      "  0.83526316  0.8377193   0.84017544  0.84263158  0.84508772  0.84754386\n",
      "  0.85        0.85245614  0.85491228  0.85736842  0.85982456  0.8622807\n",
      "  0.86473684  0.86719298  0.86964912  0.87210526  0.8745614   0.87701754\n",
      "  0.87947368  0.88192982  0.88438596  0.88684211  0.88929825  0.89175439\n",
      "  0.89421053  0.89666667  0.89912281  0.90157895  0.90403509  0.90649123\n",
      "  0.90894737  0.91140351  0.91385965  0.91631579  0.91877193  0.92122807\n",
      "  0.92368421  0.92614035  0.92859649  0.93105263  0.93350877  0.93596491\n",
      "  0.93842105  0.94087719  0.94333333  0.94578947  0.94824561  0.95070175\n",
      "  0.95315789  0.95561404  0.95807018  0.96052632  0.96298246  0.9654386\n",
      "  0.96789474  0.97035088  0.97280702  0.97526316  0.9777193   0.98017544\n",
      "  0.98263158  0.98508772  0.98754386  0.99      ]\n"
     ]
    }
   ],
   "source": [
    "rows = [0, 1, 2]\n",
    "min_alpha = 0.01\n",
    "max_alpha = 0.99\n",
    "num_alphas = 100\n",
    "alphas = np.linspace(min_alpha, max_alpha, num_alphas)\n",
    "print alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV is  sklearn.cross_validation.StratifiedKFold(labels=[0 0 0 1 1 1 2 2 2 3], n_folds=3, shuffle=False, random_state=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CausalTests.py:356: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  topk_indices = sorted(np.argpartition(a, k)[:k], key = lambda entry: a[entry])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV is  sklearn.cross_validation.StratifiedKFold(labels=[0 0 0 1 1 1 2 2 2 3], n_folds=3, shuffle=False, random_state=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CausalTests.py:258: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if header == None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV is  sklearn.cross_validation.StratifiedKFold(labels=[0 0 0 1 1 1 2 2 2 3], n_folds=3, shuffle=False, random_state=None)\n",
      "CPU times: user 1min 13s, sys: 502 ms, total: 1min 14s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%time betas, all_res_df, use_df = ct.enet_granger_causality_row(geneTS, geneTS, rows, alphas )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data, 3000 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg + STD loaded\n",
      "(3545, 12)\n",
      "[[ 1.47040223  1.4982487   1.47821205 ...,  1.53996098  1.29676846\n",
      "   1.44062158]\n",
      " [ 1.76348224  1.81127374  1.82762428 ...,  1.99445511  1.99816954\n",
      "   1.94426186]\n",
      " [ 4.10832635  4.1072434   4.27356973 ...,  4.29147524  4.35970052\n",
      "   4.35678656]\n",
      " ..., \n",
      " [ 1.58209541  1.64840443  1.79267322 ...,  1.88709416  1.86847499\n",
      "   1.87976474]\n",
      " [ 9.7814493   9.60533985  9.76557269 ...,  9.95068726  9.85221996\n",
      "   9.87138881]\n",
      " [ 9.15629864  8.9766435   9.25240276 ...,  9.45575629  9.38915071\n",
      "   9.37575346]]\n"
     ]
    }
   ],
   "source": [
    "data = gtm.load_file_and_avg(\"../data/GeneExpressionData/edgeR-reg-avg/featurecounts.genes.TPM.selected_reps.ln.surrogate_variables_corrected.protein_coding-edgeR-reg-avg.txt\")\n",
    "\n",
    "found_genes, geneTS = gtm.get_gene_TS(data, data[\"gene\"])\n",
    "\n",
    "print geneTS.shape\n",
    "print geneTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01        0.06157895  0.11315789  0.16473684  0.21631579  0.26789474\n",
      "  0.31947368  0.37105263  0.42263158  0.47421053  0.52578947  0.57736842\n",
      "  0.62894737  0.68052632  0.73210526  0.78368421  0.83526316  0.88684211\n",
      "  0.93842105  0.99      ]\n"
     ]
    }
   ],
   "source": [
    "rows = [0]\n",
    "min_alpha = 0.01\n",
    "max_alpha = 0.99\n",
    "num_alphas = 20\n",
    "alphas = np.linspace(min_alpha, max_alpha, num_alphas)\n",
    "print alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'CausalTests' from 'CausalTests.py'>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV is  sklearn.cross_validation.StratifiedKFold(labels=[0 0 0 1 1 1 2 2 2 3], n_folds=3, shuffle=False, random_state=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CausalTests.py:364: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  def get_CCM(X_list, Y_list, L, tau=1, E=3, test_indices=None, num_test=100, use_same=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of beta tuple is  (1, 3545, 2)\n",
      "CPU times: user 40.7 s, sys: 377 ms, total: 41 s\n",
      "Wall time: 41.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CausalTests.py:270: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "%time beta_tuple, all_res_df, use_df = ct.enet_granger_causality_row(geneTS, geneTS, rows, alphas )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7090)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>error.min</th>\n",
       "      <th>lambda.min</th>\n",
       "      <th>test_err</th>\n",
       "      <th>score</th>\n",
       "      <th>df</th>\n",
       "      <th>Row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.430776</td>\n",
       "      <td>0.048229</td>\n",
       "      <td>0.076822</td>\n",
       "      <td>0.005810</td>\n",
       "      <td>0.647708</td>\n",
       "      <td>27.414000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.313014</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.129277</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.103213</td>\n",
       "      <td>63.237327</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.040562</td>\n",
       "      <td>0.011350</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.461370</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.108990</td>\n",
       "      <td>0.048159</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.005338</td>\n",
       "      <td>0.579004</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.435657</td>\n",
       "      <td>0.048655</td>\n",
       "      <td>0.031812</td>\n",
       "      <td>0.006075</td>\n",
       "      <td>0.631622</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.695505</td>\n",
       "      <td>0.049035</td>\n",
       "      <td>0.076082</td>\n",
       "      <td>0.006943</td>\n",
       "      <td>0.676351</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.049563</td>\n",
       "      <td>1.291974</td>\n",
       "      <td>0.008883</td>\n",
       "      <td>0.997972</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             alpha    error.min   lambda.min     test_err        score  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.430776     0.048229     0.076822     0.005810     0.647708   \n",
       "std       0.313014     0.001502     0.129277     0.001702     0.103213   \n",
       "min       0.010000     0.040562     0.011350     0.000033     0.461370   \n",
       "25%       0.108990     0.048159     0.020110     0.005338     0.579004   \n",
       "50%       0.435657     0.048655     0.031812     0.006075     0.631622   \n",
       "75%       0.695505     0.049035     0.076082     0.006943     0.676351   \n",
       "max       0.990000     0.049563     1.291974     0.008883     0.997972   \n",
       "\n",
       "                df     Row  \n",
       "count  1000.000000  1000.0  \n",
       "mean     27.414000     0.0  \n",
       "std      63.237327     0.0  \n",
       "min       2.000000     0.0  \n",
       "25%       3.000000     0.0  \n",
       "50%       5.000000     0.0  \n",
       "75%      15.000000     0.0  \n",
       "max     388.000000     0.0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_res_df.sort(\"test_err\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_dfs = []\n",
    "use_dfs.append(use_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_dfs.append(use_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[  alpha  error.min lambda.min     test_err     score   df Row\n",
       " 0  0.01  0.0494822  0.0242092  3.34374e-05  0.997972  388   0,\n",
       "   alpha  error.min lambda.min     test_err     score   df Row\n",
       " 0  0.01  0.0492037  0.0298463  5.03723e-05  0.996946  369   0]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = 3\n",
    "y = geneTS[row].T\n",
    "X = np.concatenate((geneTS[:row], [geneTS[row]], geneTS[row+1:])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lag = 2\n",
    "n, T = np.array(geneTS).shape\n",
    "\n",
    "X_t = np.zeros((T - lag, n * lag))\n",
    "\n",
    "for j in range(T - lag):\n",
    "    for k in range(lag):\n",
    "        X_t[j, n*k:n*(k+1)] = X[j + lag - k - 1, ]\n",
    "\n",
    "# Y is a vector of T -lag rows where for row j, the value of j is i = j + lag\n",
    "y_t = y[lag:T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.77824874  0.80234869  0.79111867  0.69111471  0.7691065   0.77267344\n",
      "  0.51252858  0.61420831  0.75277127  0.82126244  0.77585586  0.78346855]\n",
      "[ 0.79111867  0.69111471  0.7691065   0.77267344  0.51252858  0.61420831\n",
      "  0.75277127  0.82126244  0.77585586  0.78346855]\n"
     ]
    }
   ],
   "source": [
    "print y\n",
    "print y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7090)\n"
     ]
    }
   ],
   "source": [
    "print X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.4982487   1.81127374]\n",
      " [ 1.47821205  1.82762428]\n",
      " [ 1.40595194  1.8507007 ]\n",
      " [ 1.14554714  1.79869023]\n",
      " [ 1.36724846  1.76622767]\n",
      " [ 1.12762626  1.70286016]\n",
      " [ 1.29195305  1.9233287 ]\n",
      " [ 1.40745521  1.87613439]\n",
      " [ 1.53996098  1.99445511]\n",
      " [ 1.29676846  1.99816954]]\n",
      "[[ 1.47040223  1.76348224]\n",
      " [ 1.4982487   1.81127374]\n",
      " [ 1.47821205  1.82762428]\n",
      " [ 1.40595194  1.8507007 ]\n",
      " [ 1.14554714  1.79869023]\n",
      " [ 1.36724846  1.76622767]\n",
      " [ 1.12762626  1.70286016]\n",
      " [ 1.29195305  1.9233287 ]\n",
      " [ 1.40745521  1.87613439]\n",
      " [ 1.53996098  1.99445511]]\n"
     ]
    }
   ],
   "source": [
    "print X_t[:, 0:2]\n",
    "print X_t[:, 3545: 3547]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "betas = pickle.load(open(\"small_TPM-avg-betas.p\",'rU'))\n",
    "print betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = [0.5]\n",
    "lambda_mins = [0.01]\n",
    "df = pd.DataFrame({\"alpha\": alphas, \"lambda.min\": lambda_mins})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirming elastic net scrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg + STD loaded\n"
     ]
    }
   ],
   "source": [
    "df = gtm.load_file_and_avg(\"small_TPM-avg.txt\")\n",
    "genes = df['gene'].values\n",
    "\n",
    "found_genes, geneTS = gtm.get_gene_TS(df, genes)\n",
    "\n",
    "geneTS.shape\n",
    "\n",
    "matr1 = geneTS\n",
    "matr2 = geneTS\n",
    "model_order = 2\n",
    "n, T = np.array(matr1).shape\n",
    "lag = model_order\n",
    "row = 0\n",
    "\n",
    "y = matr2[row].T\n",
    "\n",
    "X = np.concatenate((matr1[:row], [matr2[row]], matr1[row+1:])).T\n",
    "\n",
    "X_t = np.zeros((T - lag, n * lag))\n",
    "\n",
    "for j in range(T - lag):\n",
    "    for k in range(lag):\n",
    "        X_t[j, n*k:n*(k+1)] = X[j + lag - k - 1, ]\n",
    "\n",
    "# Y is a vector of T -lag rows where for row j, the value of j is i = j + lag\n",
    "y_t = y[lag:T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geneTSmunging as gtm\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_coef = pickle.load(open(\"small_TPM-avg-betas.p\", 'rU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "print test_coef.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.          0.        ]\n",
      " [ 0.08798916 -0.        ]\n",
      " [ 0.01260309 -0.        ]\n",
      " [-0.         -0.        ]\n",
      " [ 0.         -0.        ]\n",
      " [-0.         -0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.         -0.        ]\n",
      " [ 0.5569271  -0.08937858]\n",
      " [ 0.         -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print test_coef[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>error.min</th>\n",
       "      <th>lambda.min</th>\n",
       "      <th>test_err</th>\n",
       "      <th>score</th>\n",
       "      <th>df</th>\n",
       "      <th>Row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.050032</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999837</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999789</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.050583</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.999160</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.034819</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999824</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.035037</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999767</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  error.min  lambda.min  test_err     score    df  Row\n",
       "0    0.5   0.050032      0.0001  0.000002  0.999837  16.0  0.0\n",
       "1    0.6   0.049103      0.0001  0.000003  0.999789  13.0  0.0\n",
       "2    0.6   0.050583      0.0002  0.000011  0.999160  12.0  0.0\n",
       "3    0.5   0.034819      0.0001  0.000002  0.999824  16.0  1.0\n",
       "4    0.6   0.035037      0.0001  0.000002  0.999767  11.0  1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_res = pd.read_csv(\"small_TPM-avg-all.df\", sep=\"\\t\")\n",
    "all_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test err:  2.18589326755e-06\n"
     ]
    }
   ],
   "source": [
    "enet = ElasticNet(l1_ratio=0.5, alpha=0.0001, fit_intercept=True, normalize=False, tol=0.0000001, max_iter = 100000)\n",
    "enet.fit(X_t, y_t)\n",
    "\n",
    "y_pred =enet.predict(X_t)\n",
    "test_err = np.average((y_t - y_pred)**2)\n",
    "score = enet.score(X_t, y_t)\n",
    "test_coef = enet.coef_\n",
    "print \"Test err: \", test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  1,   2,   8,  17,  71,  84,  85,  86,  99, 108, 117, 125, 166,\n",
      "       173, 188, 193]),)\n",
      "[[[-0.          0.        ]\n",
      "  [ 0.08923267 -0.        ]\n",
      "  [ 0.00709274 -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.55916556 -0.09027642]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.30767606  0.03783378]\n",
      "  [-0.          0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [ 0.         -0.01720717]\n",
      "  [ 0.          0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.         -0.16536249]\n",
      "  [-0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.08245924 -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.         -0.10547125]\n",
      "  [-0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [-0.12459674  0.        ]\n",
      "  [-0.08606583 -0.        ]\n",
      "  [-0.00322931 -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [-0.         -0.0398724 ]\n",
      "  [-0.         -0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [ 0.         -0.00595422]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.         -0.        ]\n",
      "  [-0.         -0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [-0.          0.        ]\n",
      "  [ 0.20549325 -0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print np.where(enet.coef_)\n",
    "\n",
    "print np.dstack(tuple(np.split(np.array([enet.coef_]), lag, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirm cross-validation for enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lag: 1\n",
      "(array([ 1,  2,  4,  5,  7,  8, 10]), array([0, 3, 6, 9]))\n",
      "(array([0, 2, 3, 5, 6, 8, 9]), array([ 1,  4,  7, 10]))\n",
      "(array([ 0,  1,  3,  4,  6,  7,  9, 10]), array([2, 5, 8]))\n",
      "For lag: 2\n",
      "(array([1, 2, 4, 5, 7, 8]), array([0, 3, 6, 9]))\n",
      "(array([0, 2, 3, 5, 6, 8, 9]), array([1, 4, 7]))\n",
      "(array([0, 1, 3, 4, 6, 7, 9]), array([2, 5, 8]))\n",
      "For lag: 3\n",
      "(array([1, 2, 4, 5, 7, 8]), array([0, 3, 6]))\n",
      "(array([0, 2, 3, 5, 6, 8]), array([1, 4, 7]))\n",
      "(array([0, 1, 3, 4, 6, 7]), array([2, 5, 8]))\n"
     ]
    }
   ],
   "source": [
    "import CausalTests as ct\n",
    "lags = [1,2,3]\n",
    "for lag in lags:\n",
    "    print \"For lag:\" , lag\n",
    "    for i in ct.get_TS_cv(12-lag, 3):\n",
    "        print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Confirm using same params method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geneTSmunging as gtm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import CausalTests as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg + STD loaded\n",
      "Avg + STD loaded\n"
     ]
    }
   ],
   "source": [
    "df = gtm.load_file_and_avg(\"../data/GeneExpressionData/small_TPM-avg.txt\")\n",
    "dfr = gtm.load_file_and_avg(\"../data/GeneExpressionData/small_TPM-randomized-avg.txt\")\n",
    "genes = df['gene'].values\n",
    "\n",
    "found_genes, geneTS = gtm.get_gene_TS(df, genes)\n",
    "\n",
    "found_genesr, geneTSr = gtm.get_gene_TS(dfr, dfr[\"gene\"].values)\n",
    "\n",
    "matr1 = geneTS\n",
    "matr2 = geneTSr\n",
    "model_order = 2\n",
    "n, T = np.array(matr1).shape\n",
    "lag = model_order\n",
    "rowlist = [0, 4]\n",
    "\n",
    "args_dict = ct.load_kwargs_file(argsfile=\"enet_args-2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/cross_validation.py:417: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV is  sklearn.cross_validation.StratifiedKFold(labels=[0 0 0 1 1 1 2 2 2 3], n_folds=3, shuffle=False, random_state=None)\n",
      "Num cv alphas:  2000\n",
      "Top num  1000\n",
      "CV is  sklearn.cross_validation.StratifiedKFold(labels=[0 0 0 1 1 1 2 2 2 3], n_folds=3, shuffle=False, random_state=None)\n",
      "Num cv alphas:  2000\n",
      "Top num  1000\n",
      "Shape of beta tuple is  (2, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "beta_tuple, all_res_df, use_df = ct.enet_granger_causality_row_cv(geneTS, geneTS, rowlist, **args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>error.min</th>\n",
       "      <th>lambda.min</th>\n",
       "      <th>test_err</th>\n",
       "      <th>score</th>\n",
       "      <th>df</th>\n",
       "      <th>Row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.521579</td>\n",
       "      <td>0.054176</td>\n",
       "      <td>2.70023e-05</td>\n",
       "      <td>1.6977e-07</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0216294</td>\n",
       "      <td>1.08481e-05</td>\n",
       "      <td>4.14845e-08</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha  error.min   lambda.min     test_err     score  df Row\n",
       "0  0.521579   0.054176  2.70023e-05   1.6977e-07  0.999987  15   0\n",
       "1      0.99  0.0216294  1.08481e-05  4.14845e-08  0.999995  11   4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_df = use_df[[\"alpha\", \"lambda.min\", \"Row\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of beta tuple is  (2, 100, 2)\n",
      "      alpha   lambda.min Row     test_err     score  df\n",
      "0  0.521579  2.70023e-05   0  4.30082e-08  0.999997  11\n",
      "1      0.99  1.08481e-05   4   2.5596e-08  0.999997  11\n",
      "      alpha   lambda.min Row     test_err     score  df\n",
      "0  0.521579  2.70023e-05   0  4.30082e-08  0.999997  11\n",
      "1      0.99  1.08481e-05   4   2.5596e-08  0.999997  11\n"
     ]
    }
   ],
   "source": [
    "rand_beta_tuple, rand_all_res_df, rand_use_df = ct.enet_granger_causality_row_load(geneTSr, geneTS, rowlist, param_df,**args_dict )\n",
    "\n",
    "print rand_all_res_df.head()\n",
    "print rand_use_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_res_dft = pd.read_csv(\"Enet/test-small-TPM-enet-all-params.txt\", sep=\"\\t\")\n",
    "use_dft = pd.read_csv(\"Enet/test-small-TPM-enet-use-params.txt\", sep=\"\\t\")\n",
    "rand_all_res_dft = pd.read_csv(\"Enet/test-small-TPM-enet-all-params-randomized.txt\", sep=\"\\t\")\n",
    "rand_use_dft = pd.read_csv(\"Enet/test-small-TPM-enet-use-params-randomized.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 7) (100000, 7) (2000, 7)\n",
      "     alpha  error.min   lambda.min     test_err     score  df Row\n",
      "1032   0.1  0.0368158  0.000102578  1.20316e-06   0.99988  37   4\n",
      "1038   0.1  0.0368136  0.000109991  1.38157e-06  0.999862  37   4\n",
      "1044   0.1  0.0368129  0.000117939  1.58629e-06  0.999841  37   4\n",
      "1050   0.1  0.0368105  0.000126463  1.82122e-06  0.999818  37   4\n",
      "1056   0.1  0.0368089  0.000135602  2.09075e-06  0.999791  37   4\n",
      "      alpha  error.min  lambda.min  test_err     score    df  Row\n",
      "4032    0.1   0.036816    0.000103  0.000001  0.999880  37.0  4.0\n",
      "4038    0.1   0.036814    0.000110  0.000001  0.999862  37.0  4.0\n",
      "4044    0.1   0.036813    0.000118  0.000002  0.999841  37.0  4.0\n",
      "4050    0.1   0.036810    0.000126  0.000002  0.999818  37.0  4.0\n",
      "4056    0.1   0.036809    0.000136  0.000002  0.999791  37.0  4.0\n",
      "(2, 7) (100, 7) (2, 7)\n",
      "      alpha  error.min   lambda.min     test_err     score  df Row\n",
      "1  0.334211  0.0454934  3.06926e-05   3.5283e-07  0.999965  20   4\n",
      "0  0.334211   0.042426  4.91357e-05  7.96538e-07  0.999936  22   0\n",
      "      alpha  error.min  lambda.min      test_err     score    df  Row\n",
      "4  0.334211   0.045493    0.000031  3.528302e-07  0.999965  20.0  4.0\n",
      "0  0.334211   0.042426    0.000049  7.965378e-07  0.999936  22.0  0.0\n",
      "(2, 6) (100, 6) (2, 6)\n",
      "      alpha   lambda.min Row     test_err     score  df\n",
      "1  0.334211  3.06926e-05   4   3.6297e-08  0.999996  15\n",
      "0  0.334211  4.91357e-05   0  7.13761e-08  0.999995  12\n",
      "      alpha  lambda.min  Row      test_err     score    df\n",
      "4  0.334211    0.000031  4.0  3.629699e-08  0.999996  15.0\n",
      "0  0.334211    0.000049  0.0  7.137608e-08  0.999995  12.0\n",
      "(2, 6) (100, 6) (2, 6)\n",
      "      alpha   lambda.min Row     test_err     score  df\n",
      "1  0.334211  3.06926e-05   4   3.6297e-08  0.999996  15\n",
      "0  0.334211  4.91357e-05   0  7.13761e-08  0.999995  12\n",
      "      alpha  lambda.min  Row      test_err     score    df\n",
      "4  0.334211    0.000031  4.0  3.629699e-08  0.999996  15.0\n",
      "0  0.334211    0.000049  0.0  7.137608e-08  0.999995  12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "orig = [all_res_df, use_df, rand_all_res_df, rand_use_df]\n",
    "test = [all_res_dft, use_dft, rand_all_res_dft, rand_use_dft]\n",
    "\n",
    "comps = zip(orig, test)\n",
    "\n",
    "for dfo, dft in comps:\n",
    "    dfot = dft[dft[\"Row\"].isin({0,4})]\n",
    "    dfo.sort_values([\"alpha\", \"lambda.min\"], inplace=True)\n",
    "    dfot.sort_values([\"alpha\", \"lambda.min\"], inplace=True)\n",
    "    print dfo.shape, dft.shape, dfot.shape\n",
    "    print dfo.head()\n",
    "    print dfot.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the same params were used, confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_tuplet = pickle.load(open(\"Enet/test-small-TPM-enet.p\", 'rB'))\n",
    "rand_beta_tuplet = pickle.load(open(\"Enet/test-small-TPM-enet-randomized.p\", 'rB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_tuplet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_tuple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [False False]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [False False]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True False]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True False]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [False  True]\n",
      " [ True  True]\n",
      " [ True False]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True False]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True False]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [False  True]]\n",
      "[[ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]]\n"
     ]
    }
   ],
   "source": [
    "print beta_tuplet[0] == beta_tuple[0]\n",
    "print beta_tuplet[4] == beta_tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_beta_tuple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_beta_tuplet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[[  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  2.77555756e-16   4.99600361e-16]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00  -6.31439345e-16]\n",
      " [ -3.88578059e-16   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00  -9.57567359e-16]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  1.11022302e-16   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [ -1.32185929e-15   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   3.19189120e-16]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00  -2.77555756e-17]\n",
      " [  4.92661467e-16   0.00000000e+00]\n",
      " [  0.00000000e+00   6.93889390e-18]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print rand_beta_tuplet[0] - rand_beta_tuple[0]\n",
    "print rand_beta_tuplet[4] - rand_beta_tuple[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
